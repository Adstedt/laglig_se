# Story 14.6: File Knowledge Extraction & Embedding

## Status

Draft

## Story

**As a** workspace member who has uploaded company policies and evidence,
**I want** the compliance partner to understand the content of my uploaded files,
**so that** it can reference my actual policies when giving guidance.

## Context & Dependencies

Users already upload "bilagor" and "bevis" (attachments/evidence) via the file management system (Stories 6.7a, 6.7b). This story adds text extraction, AI classification, and embedding so the agent can search and reference these files. File content is workspace-private — never visible to other workspaces.

- **Builds on:** Existing file management from Stories 6.7a, 6.7b (`app/actions/file.ts`, `components/features/files/`)
- **Depends on:** 14.2 (ContentChunk model), 14.3 (embedding pipeline)
- **Depended on by:** 14.7 (search_company_files tool), 14.8 (RAG includes file chunks)

## Acceptance Criteria

### Text Extraction

1. Text extraction for PDF files using existing pdf-parse package
2. Text extraction for Word documents (.docx) — add appropriate library
3. Extracted text stored in new `extractedText` field on File model
4. Extraction runs as async background job on file upload
5. Re-extraction available via manual trigger ("Bearbeta om" button)

### AI Classification

6. AI classifies uploaded file into categories: POLICY, RISK_ASSESSMENT, CERTIFICATE, PROCEDURE, AUDIT_REPORT, TRAINING_MATERIAL, OTHER
7. AI generates a 1-2 sentence content summary stored in `contentSummary` field
8. Classification stored in new `documentCategory` field on File model
9. Classification runs after text extraction

### Chunking & Embedding

10. Extracted text chunked and stored in ContentChunk with sourceType = USER_FILE
11. Chunks have workspaceId set (never NULL — file content is workspace-private)
12. Chunks include contextual header: "Foretagets dokument: [filename] > [section/page]"
13. Embeddings generated for file chunks using same pipeline as Story 14.3
14. When a file is deleted, its chunks are cascade-deleted

### File Model Changes

15. Add `extractedText` (Text, nullable), `contentSummary` (String, nullable), `documentCategory` (enum, nullable) to File model
16. Add FileDocumentCategory enum: POLICY, RISK_ASSESSMENT, CERTIFICATE, PROCEDURE, AUDIT_REPORT, TRAINING_MATERIAL, OTHER
17. Migration runs cleanly

### Testing

18. At least 8 unit tests covering extraction, classification, chunking

## Tasks / Subtasks

- [ ] **Task 1: Schema changes** (AC: 15-17)
  - [ ] Add `extractedText` (Text, nullable) to File model
  - [ ] Add `contentSummary` (String, nullable) to File model
  - [ ] Add `documentCategory` (FileDocumentCategory, nullable) to File model
  - [ ] Add `FileDocumentCategory` enum with all values
  - [ ] Generate and run migration
  - [ ] Verify migration is non-breaking (all new fields nullable)

- [ ] **Task 2: Text extraction** (AC: 1-3)
  - [ ] Create `lib/files/extract-text.ts`
  - [ ] PDF extraction via existing `pdf-parse` package
  - [ ] Word extraction via `mammoth` (add to dependencies)
  - [ ] Return plain text string from extraction
  - [ ] Handle extraction errors gracefully (corrupted files, password-protected PDFs)
  - [ ] Save extracted text to File.extractedText

- [ ] **Task 3: AI classification** (AC: 6-9)
  - [ ] Create `lib/files/classify-document.ts`
  - [ ] Send first ~2000 characters of extracted text to Claude
  - [ ] Prompt: classify into FileDocumentCategory + generate 1-2 sentence Swedish summary
  - [ ] Parse AI response and save category + summary to File model
  - [ ] Handle classification failures gracefully (default to OTHER)

- [ ] **Task 4: File processing pipeline** (AC: 4-5)
  - [ ] Create `lib/files/process-uploaded-file.ts`
  - [ ] Orchestrate: extract text -> classify -> chunk -> embed
  - [ ] Call pipeline after successful file upload in `app/actions/file.ts`
  - [ ] Async execution (don't block upload response)
  - [ ] Add processing status tracking (optional: `processingStatus` field)

- [ ] **Task 5: Chunking** (AC: 10-13)
  - [ ] Reuse chunking logic from Story 14.2 adapted for file content
  - [ ] Split by page breaks or ~500 token chunks
  - [ ] Set sourceType = USER_FILE, sourceId = file ID
  - [ ] Always set workspaceId on every chunk
  - [ ] Add contextual header: "Foretagets dokument: [filename] > [section/page]"

- [ ] **Task 6: Cascade deletion** (AC: 14)
  - [ ] When file is deleted via `app/actions/file.ts`, delete all ContentChunks with sourceType=USER_FILE and sourceId=fileId
  - [ ] Ensure deletion is transactional (file + chunks deleted together or not at all)

- [ ] **Task 7: UI updates**
  - [ ] Add document category badge to file list items in `components/features/files/`
  - [ ] Add "Bearbeta om" (re-process) button to file detail/actions
  - [ ] Show content summary as tooltip or expandable text
  - [ ] Show processing status indicator during extraction

- [ ] **Task 8: Tests** (AC: 18)
  - [ ] Write unit tests for PDF text extraction
  - [ ] Write unit tests for Word text extraction
  - [ ] Write unit tests for AI classification
  - [ ] Write unit tests for chunking with contextual headers
  - [ ] Write unit tests for cascade deletion
  - [ ] Minimum 8 tests total

## Dev Notes

- Existing File model at `prisma/schema.prisma` — extend with new fields
- Existing file upload: `app/actions/file.ts`, Supabase Storage for binary storage
- `pdf-parse` already in `package.json` (used for amendment PDF processing)
- For Word docs: use `mammoth` (converts .docx to HTML/text, lightweight). Install with `pnpm add mammoth`.
- AI classification prompt example:
  > "Klassificera detta dokument som en av: POLICY, RISK_ASSESSMENT, CERTIFICATE, PROCEDURE, AUDIT_REPORT, TRAINING_MATERIAL, OTHER. Ge ocksa en sammanfattning pa 1-2 meningar pa svenska."
- File chunks are ALWAYS workspace-scoped — set `workspaceId` on every ContentChunk. This is critical for data isolation.
- Contextual header format: `"Foretagets dokument: Arbetsmiljopolicy.pdf > Sida 3"`
- Background processing: for now, call `processUploadedFile()` after successful upload. Can be moved to a queue (e.g., Inngest, pg-boss) later if needed.
- Download file from Supabase Storage for extraction: check `lib/supabase/storage.ts` for existing download helpers
- Embedding pipeline from Story 14.3 should expose a function like `generateEmbeddings(chunks)` — reuse it here
- ContentChunk model from Story 14.2 should have fields: sourceType, sourceId, workspaceId, content, embedding, contextHeader
- Source tree:
  - `prisma/schema.prisma` — extend File model, add FileDocumentCategory enum
  - `app/actions/file.ts` (existing) — hook into upload flow, add cascade deletion
  - `lib/files/extract-text.ts` (new) — text extraction
  - `lib/files/classify-document.ts` (new) — AI classification
  - `lib/files/process-uploaded-file.ts` (new) — orchestration pipeline
  - `components/features/files/` (existing) — UI updates

## Testing

- **Test files:**
  - `tests/unit/files/extract-text.test.ts`
  - `tests/unit/files/classify-document.test.ts`
  - `tests/unit/files/process-uploaded-file.test.ts`
- **What to test:**
  - PDF extraction returns correct text content
  - PDF extraction handles empty/corrupted files gracefully
  - Word extraction returns correct text content
  - AI classification returns valid category and summary
  - AI classification defaults to OTHER on failure
  - Chunking splits text into ~500 token chunks with correct contextual headers
  - Chunks have workspaceId and sourceType=USER_FILE set
  - Cascade deletion removes all chunks when file is deleted
- **Patterns to follow:** Mock `pdf-parse` for PDF extraction tests. Mock `mammoth` for Word extraction tests. Mock AI API for classification tests. Mock Prisma client for storage/deletion tests. Use Vitest with `vi.mock`.

## Change Log

| Date       | Version | Description                    | Author |
|------------|---------|--------------------------------|--------|
| 2026-02-18 | 0.1     | Initial draft                  | Claude |

## Dev Agent Record

### Agent Model Used

### Debug Log References

### Completion Notes List

### File List

## QA Results
