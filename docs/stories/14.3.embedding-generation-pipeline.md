# Story 14.3: Embedding Generation Pipeline

## Status

Ready for Review

## Story

**As a** platform preparing for semantic search,
**I want** high-quality contextual embeddings generated for all content chunks across the database,
**so that** the RAG pipeline can perform accurate vector similarity search with full document context.

## Context & Dependencies

**Why this story exists:**

Story 14.2 creates the `ContentChunk` model and populates it with paragraph-level chunks derived from `json_content`. But those chunks have `embedding = NULL` — they cannot be searched by vector similarity. This story builds two pipelines:

1. **LLM Contextual Retrieval** — Before embedding, each chunk gets a short (50-100 token) semantic context prefix generated by an LLM. Based on [Anthropic's contextual retrieval research](https://www.anthropic.com/news/contextual-retrieval), this reduces retrieval failures by up to 67%. Instead of sending one API call per chunk (250K+ calls), we send **one call per document** containing all its chunks, producing all context prefixes at once (~11K calls total).

2. **Embedding Generation** — The contextualized text (prefix + content) is embedded using OpenAI's `text-embedding-3-small` (1536 dims), stored in the `ContentChunk.embedding` column with HNSW index.

After this story, the database is ready for vector similarity search with high-quality contextual embeddings.

**Builds on:**

- Story 14.2 (ContentChunk Model & Chunking Pipeline) — provides `ContentChunk` records with `content`, `contextual_header`, and `embedding = NULL`
- `ContentChunk.embedding` — `Unsupported("vector(1536)")` field, HNSW index already created by Story 14.2
- OpenAI SDK — already in `package.json` (used for existing AI features like `ai-summary-queue.ts`) [Source: architecture/3-tech-stack.md#AI-Embeddings]
- Anthropic SDK — already in `package.json` (used for amendment processing). Claude Haiku used for context prefix generation
- `LegalDocument.embedding` — existing full-document embedding using same model [Source: prisma/schema.prisma:301]
- `LegalDocument.markdown_content` — used as document context for LLM contextual retrieval
- Embedding model confirmed: OpenAI `text-embedding-3-small` (1536 dimensions) [Source: architecture/3-tech-stack.md#32-notable-technology-decisions]
- Token count available on each chunk (`ContentChunk.token_count`) from Story 14.2

**Depends on:** Story 14.2 must be complete (ContentChunk table must exist with HNSW index and populated chunks).

**Manual prerequisite:** Bulk chunking of all ~11.4K SFS_LAW + ~288 AGENCY_REGULATION documents must be run before this story's batch script. Use `syncDocumentChunks()` from Story 14.2 to populate all ContentChunk records first. The embedding script (Task 4) assumes chunks already exist — it does NOT orchestrate chunking.

**Key data points (from DB analysis):**

| Document size (markdown tokens) | Count | Strategy |
|---|---|---|
| <10K tokens | 10,150 (89%) | Send full markdown in one call |
| 10-50K tokens | 1,098 | Send full markdown in one call |
| 50-200K tokens | 137 | Send full markdown in one call |
| >200K tokens | 13 | Split at division/avdelning level |

Total SFS_LAW: ~11,398 documents. Plus AGENCY_REGULATION documents.
Markdown is ~50% the size of JSON — all but 13 laws fit within a 200K token context window.

**Depended on by:**

- All RAG retrieval stories — vector similarity queries require embeddings to exist
- Story 14.4 (Workspace Profile Data Model) and Story 14.8 (RAG Retrieval Pipeline) — queries ContentChunk with `<=>` cosine distance operator
- Future incremental sync stories — new documents go through chunk + context + embed pipeline

## Acceptance Criteria

### LLM Context Prefix Generation (Contextual Retrieval)

1. Context generation function in `lib/chunks/generate-context-prefixes.ts` takes a document's markdown + all its chunks and returns a context prefix (50-100 tokens) per chunk
2. Uses Claude Haiku via Anthropic SDK — one API call per document, all chunks in one request
3. For documents where markdown exceeds 200K tokens (~13 SFS laws), split context at division/avdelning level; if division still exceeds limit, fall back to chapter level
4. Context prefix stored in `ContentChunk.context_prefix` field (new column, `Text`, nullable)
5. The stored `contextual_header` (structural breadcrumb) is separate from the LLM-generated `context_prefix` (semantic summary) — both are prepended before embedding

### Embedding Function

6. Embedding model: OpenAI `text-embedding-3-small` (1536 dimensions) [Source: architecture/3-tech-stack.md#AI-Embeddings]
7. Embedding function in `lib/chunks/embed-chunks.ts` takes chunk content + context prefix + contextual header and returns a 1536-dim embedding vector
8. Embedding input combines: `contextual_header + "\n" + context_prefix + "\n\n" + content`

### Batch Script

9. Batch script `scripts/generate-embeddings.ts` orchestrates: (a) generate context prefixes for all documents, then (b) embed all contextualized chunks
10. Script has cost estimation mode (`--estimate` flag) — calculates total tokens and estimated cost for both LLM context generation (Haiku) and embedding (OpenAI) before running
11. Script processes embedding in batches of 100 (OpenAI embedding API batch limit per request), with rate limiting between batches
12. Script has progress logging: `"Embedded 5,000/150,000 chunks (3.3%) | Cost so far: $0.10"`
13. Script has resume-on-failure: tracks last processed document/chunk ID in a cursor file, can restart from where it left off with `--resume` flag

### Incremental Embedding

14. Incremental embedding: when new chunks are created via `syncDocumentChunks` (Story 14.2), context prefix + embedding are generated immediately after chunk creation

### Index Tuning & Verification

15. pgvector HNSW index parameters tuned for expected dataset size (~150K-300K chunks): `ef_construction = 64`, `m = 16` [Source: architecture/11-backend-architecture.md#1133-vector-search-pattern]
16. Verification script `scripts/verify-embeddings.ts` runs sample similarity queries and displays top-5 results with similarity scores for manual inspection

### Testing

17. At least 10 unit tests (mock Anthropic and OpenAI APIs)

## Tasks / Subtasks

- [x] **Task 1: Schema migration — add `context_prefix` column** (AC: 4)
  - [x] Add `context_prefix String? @db.Text` to `ContentChunk` model in `prisma/schema.prisma`
  - [x] The field name `context_prefix` already follows the snake_case convention used throughout the model (e.g., `source_type`, `workspace_id`, `token_count`) — no `@map()` needed
  - [x] Create migration: `ALTER TABLE content_chunks ADD COLUMN context_prefix TEXT`
  - [x] Run migration against dev DB
  - [x] Note: follow same Prisma migration patterns from Story 14.2 (`@id @default(cuid())`, `@@map("content_chunks")`). If `prisma migrate dev` hits P3006 shadow DB error, use `prisma db push` + manual SQL + `prisma migrate resolve --applied`

- [x] **Task 2: LLM context prefix generation function** (AC: 1-3, 5)
  - [x] Create `lib/chunks/generate-context-prefixes.ts`
  - [x] Implement `generateContextPrefixes(document: { markdown: string, title: string, documentNumber: string }, chunks: Array<{ path: string, content: string }>): Promise<Map<string, string>>`
    - Build prompt: send full markdown + list of all chunks (by path + first 200 chars)
    - Call Claude Haiku: `anthropic.messages.create({ model: 'claude-haiku-4-5-20251001', ... })`
    - Parse structured JSON response: map of chunk path → context prefix string
    - Return Map<path, prefix>
  - [x] Implement large document splitting:
    - If markdown > 200K tokens (~800K chars), load `json_content` and split at division/avdelning level
    - Each division becomes a separate API call with its own chunk subset
    - If a division still exceeds 200K, fall back to chapter-level context
  - [x] Reuse existing Anthropic SDK client pattern — the SDK (`@anthropic-ai/sdk ^0.71.2`) is already in `package.json` and used in `lib/sfs/amendment-llm-prompt.ts` for amendment processing. Follow the same instantiation pattern, or create a shared `lib/chunks/anthropic-client.ts` if a singleton is cleaner
  - [x] Handle errors: retry once on transient failure, throw on persistent failure

- [x] **Task 3: Embedding function** (AC: 6-8)
  - [x] Create `lib/chunks/embed-chunks.ts`
  - [x] Implement `generateEmbedding(text: string, contextPrefix: string, contextualHeader: string): Promise<number[]>`
    - Combine input: `const input = contextualHeader + "\n" + contextPrefix + "\n\n" + text`
    - Call OpenAI API: `openai.embeddings.create({ model: 'text-embedding-3-small', input })`
    - Return the embedding vector (`response.data[0].embedding`)
    - Handle errors: wrap in try/catch, throw descriptive error with chunk context
  - [x] Implement `generateEmbeddingsBatch(items: Array<{ text: string, contextPrefix: string, contextualHeader: string }>): Promise<number[][]>`
    - Combine inputs for all items
    - Call OpenAI API with array input: `openai.embeddings.create({ model: 'text-embedding-3-small', input: inputs })`
    - Maximum 100 items per batch call (OpenAI limit)
    - Return array of embedding vectors in same order
  - [x] Set up OpenAI client — check for existing client in `lib/sync/ai-summary-queue.ts` or create `lib/chunks/openai-client.ts` [Source: architecture/3-tech-stack.md#AI-Embeddings]

- [x] **Task 4: Batch script** (AC: 9-13)
  - [x] Create `scripts/generate-embeddings.ts`
  - [x] Accept CLI flags:
    - `--estimate` — calculate total tokens and cost for both Haiku + OpenAI, then exit
    - `--limit N` — process only first N documents
    - `--resume` — read cursor from `data/embedding-progress.json`
    - `--skip-context` — skip LLM context generation, embed with only structural header + content (useful for: testing embedding pipeline without Haiku cost, re-embedding after manual context_prefix edits, or if Anthropic API is temporarily unavailable)
    - `--source-type LEGAL_DOCUMENT` — filter by source type
  - [x] **Phase 1: Context prefix generation** (per document)
    - Query all documents that have chunks but no `context_prefix`
    - For each document: load `markdown_content`, load its chunks, call `generateContextPrefixes()`
    - Write `context_prefix` to each chunk row
    - Progress: `"[Context] 500/11,398 docs (4.4%) | Elapsed: 5m"`
  - [x] **Phase 2: Embedding generation** (per chunk batch)
    - Query all chunks where `embedding IS NULL`, ordered by id
    - Batch 100 chunks at a time → `generateEmbeddingsBatch()`
    - Write embeddings via `$executeRaw` (Prisma `Unsupported` type requires raw SQL)
    - Progress: `"[Embed] 5,000/295,000 (1.7%) | Cost: $0.10 | Elapsed: 2m"`
  - [x] Cost estimation mode (`--estimate`):
    - Haiku context: ~11K calls × avg doc size → estimate input/output tokens and cost
    - OpenAI embedding: sum `token_count` → $0.02/1M tokens
    - Print combined estimate
  - [x] Resume cursor in `data/embedding-progress.json`: track last doc (phase 1) and last chunk (phase 2)
  - [x] Error handling: log + skip on failure, abort after 5 consecutive failures

- [x] **Task 5: Incremental sync** (AC: 14)
  - [x] Modify `syncDocumentChunks` in `lib/chunks/sync-document-chunks.ts` (from Story 14.2) or create wrapper `syncDocumentChunksWithEmbeddings`
  - [x] After creating new chunks: generate context prefixes for the document, then embed all chunks
  - [x] This is synchronous (blocking) — acceptable for now since document syncs are infrequent and chunks per document are small (~10-50)
  - [x] If LLM/embedding fails, log error but do not roll back chunk creation (retryable by batch script)

- [x] **Task 6: HNSW index tuning** (AC: 15)
  - [x] Verify HNSW index parameters from Story 14.2: `m = 16`, `ef_construction = 64`
  - [x] These parameters are appropriate for ~300K vectors [Source: architecture/11-backend-architecture.md#1133-vector-search-pattern]
  - [x] Document parameter choices and query-time `ef_search` tuning in code comments
  - [x] Note: Query-time `ef_search` parameter can be set per-query via `SET hnsw.ef_search = 100`

- [x] **Task 7: Verification script** (AC: 16)
  - [x] Create `scripts/verify-embeddings.ts`
  - [x] Define 5-10 test queries in Swedish covering different legal domains:
    ```typescript
    const testQueries = [
      "arbetsgivarens skyldigheter för skyddsutrustning",       // employer PPE obligations
      "semesterersättning vid uppsägning",                      // vacation pay on termination
      "krav på ventilation i arbetslokaler",                    // ventilation requirements
      "anmälan av allvarligt olycksfall",                       // serious accident reporting
      "diskrimineringsförbudet vid anställning",                // discrimination in hiring
    ]
    ```
  - [x] For each query: embed → cosine similarity search → print top-5 with scores
  - [x] No assertions — manual inspection of retrieval quality

- [x] **Task 8: Tests** (AC: 17)
  - [x] Create `tests/unit/chunks/generate-context-prefixes.test.ts` — mock Anthropic API
  - [x] Create `tests/unit/chunks/embed-chunks.test.ts` — mock OpenAI API
  - [x] At least 10 unit tests total:
    - Context prefix: normal doc, large doc (division split), response parsing, error handling
    - Embedding: single, batch, input combining, error handling
  - [x] Update existing `tests/unit/chunks/sync-document-chunks.test.ts` to mock both Anthropic and OpenAI calls for incremental sync
  - [x] Verify `npx tsc --noEmit` passes
  - [x] Verify all existing tests still pass (3113+ tests)

## Dev Notes

### Previous Story Insights (14.2)

Key learnings from Story 14.2 that affect this story:

- **Prisma migration gotcha:** `prisma migrate dev` failed with P3006 shadow DB error due to migration sort order. Workaround: use `prisma db push` + manual migration SQL + `prisma migrate resolve --applied`. Apply same approach if migration issues arise.
- **Prisma `Unsupported` type:** Cannot use Prisma ORM to read/write `embedding` or query by it. Must use `$executeRaw` / `$queryRaw` for all vector operations.
- **Prisma `DbNull` sentinel:** Nullable JSON fields require `Prisma.DbNull` instead of `null` in `createMany`. Cast non-null JSON to `Prisma.InputJsonValue`.
- **Token count bug found in QA:** The `token_count` field must match the actual stored content (including heading if present). Be careful with similar mismatches in embedding input assembly.
- **Existing chunk data:** Story 14.2 created the chunking pipeline but no bulk chunking has been run yet. The DB currently has only 175 test chunks from manual testing. Full chunking of all ~11.4K documents needs to happen before embedding.

### Source Tree

```
lib/chunks/
  ├── chunk-document.ts                  — FROM Story 14.2
  ├── sync-document-chunks.ts            — FROM Story 14.2 — MODIFIED (add incremental context+embed)
  ├── token-count.ts                     — FROM Story 14.2
  ├── generate-context-prefixes.ts       — NEW — LLM context prefix generation (Haiku)
  ├── embed-chunks.ts                    — NEW — generateEmbedding(), generateEmbeddingsBatch()
  ├── openai-client.ts                   — NEW (if no existing client) — OpenAI SDK singleton
  └── index.ts                           — MODIFIED — add context + embedding exports

scripts/
  ├── generate-embeddings.ts             — NEW — batch context generation + embedding script
  └── verify-embeddings.ts               — NEW — verification / sanity check script

data/
  └── embedding-progress.json            — NEW — resume cursor (gitignored)
```

[Source: architecture/12-unified-project-structure.md — scripts/ for build & maintenance, lib/ for core business logic, tests/unit/ mirroring lib/ structure]

### LLM Context Prefix Generation (Anthropic Haiku)

**Approach:** One API call per document, all chunks contextualized at once. This avoids 250K+ individual calls.

**Prompt design:**
```typescript
const prompt = `Here is a Swedish legal document in markdown format:

<document>
${markdownContent}
</document>

Below are chunks extracted from this document. For each chunk, write a short context
(1-2 sentences, 50-100 tokens) that situates the chunk within the overall document.
The context should help a search engine understand what the chunk is about.

Respond as JSON: { "prefixes": { "<path>": "<context>", ... } }

Chunks:
${chunks.map(c => `[${c.path}]: ${c.content.substring(0, 200)}...`).join('\n')}
`
```

**Example output:**
```json
{
  "prefixes": {
    "kap2.§3": "Denna paragraf i Arbetsmiljölagens kapitel om arbetsmiljöns beskaffenhet specificerar arbetsgivarens ansvar för att arbetsplatsen ska vara utformad så att risker för ohälsa och olycksfall förebyggs.",
    "kap2.§4": "..."
  }
}
```

**Large document handling:**
```
if markdown_tokens < 200K → send full document
elif document has divisions → split at division level, one call per division
elif division > 200K → split at chapter level, one call per chapter
```

The 13 largest laws (Inkomstskattelagen 316K markdown tokens, Socialförsäkringsbalken 213K, etc.) all have division structure. Each division is sent as a separate API call with only its associated chunks.

**Model:** `claude-haiku-4-5-20251001`
**Pricing:** $0.80/1M input, $4/1M output (with prompt caching: $0.08/1M cached input)

### Embedding API Details

**Model:** `text-embedding-3-small` [Source: architecture/3-tech-stack.md#AI-Embeddings]
**Dimensions:** 1536
**Pricing:** $0.02 per 1M tokens
**Rate limit:** 3,000 RPM on Tier 1, 5,000 RPM on Tier 2+
**Max tokens per request:** 8,191 tokens per input
**Batch size:** Up to 2,048 inputs per request (we use 100 for safety and memory)

### Contextual Retrieval Pattern

Each chunk's embedding input combines three layers of context:

```typescript
// Embedding input for a chunk:
const embeddingInput = contextualHeader + "\n" + contextPrefix + "\n\n" + content
// Single newline between header and prefix (tight coupling),
// double newline before content (visual separation)
```

- **`contextual_header`** (from 14.2): Structural breadcrumb — free, deterministic
- **`context_prefix`** (new in 14.3): LLM-generated semantic summary — costs money, much higher quality
- **`content`**: The raw chunk text

This layered approach is based on [Anthropic's contextual retrieval research](https://www.anthropic.com/news/contextual-retrieval) — adding document context to chunks before embedding reduces retrieval failures by up to 67%.

### Writing Embeddings with Prisma

Since `embedding` uses `Unsupported("vector(1536)")`, Prisma ORM cannot read/write it directly. Use raw SQL:

```typescript
// Write embedding for a single chunk
await prisma.$executeRaw`
  UPDATE content_chunks
  SET embedding = ${vectorArrayToString(embedding)}::vector
  WHERE id = ${chunkId}
`

// Helper to convert number[] to pgvector string format
function vectorArrayToString(vector: number[]): string {
  return `[${vector.join(',')}]`
}

// Query with cosine similarity
const results = await prisma.$queryRaw`
  SELECT id, content, contextual_header,
    1 - (embedding <=> ${vectorArrayToString(queryEmbedding)}::vector) as similarity
  FROM content_chunks
  WHERE embedding IS NOT NULL
    AND source_type = 'LEGAL_DOCUMENT'
  ORDER BY embedding <=> ${vectorArrayToString(queryEmbedding)}::vector
  LIMIT ${limit}
`
```

**Important:** The `<=>` operator is cosine distance (not similarity). `similarity = 1 - distance`. Higher similarity = more relevant. [Source: architecture/11-backend-architecture.md#1133-vector-search-pattern]

### OpenAI SDK Setup

The OpenAI SDK is already in `package.json`. Check for existing client initialization in `lib/sync/ai-summary-queue.ts` — reuse existing OpenAI client setup rather than creating a duplicate. [Source: architecture/3-tech-stack.md]

### Resume Cursor Pattern

The batch script tracks progress in a JSON file to support resume-on-failure:

```typescript
// data/embedding-progress.json
{
  "phase": "context",
  "lastProcessedDocId": "24b5a04a-d5cf-...",
  "lastProcessedChunkId": "clxyz123abc",
  "totalDocsProcessed": 500,
  "totalChunksEmbedded": 5000,
  "totalCost": 0.10,
  "lastRunAt": "2026-02-25T14:30:00Z"
}
```

On `--resume`, the script reads this file and continues from the last processed document (phase 1) or chunk (phase 2). Chunk IDs are cuid (roughly time-ordered), so `WHERE id > ${lastId} ORDER BY id ASC` works for cursor pagination.

### Cost Estimation

**LLM Context Prefix Generation (one-time, Claude Haiku):**
- ~11,400 documents × avg ~15K tokens markdown = ~170M input tokens
- With prompt caching (within-document): effective cost ~$0.08/1M cached
- Output: ~11,400 docs × ~50 chunks × 75 tokens = ~43M output tokens
- **Estimated: ~$45-60** (dominated by output tokens at $4/1M)

**Embedding Generation (one-time, OpenAI):**
- ~295,000 chunks × avg ~200 tokens (header + prefix + content) = ~59M tokens
- Cost: 59M / 1M × $0.02 = **$1.18**

**Total one-time cost: ~$47-62** for full database contextualization + embedding.
**Incremental cost per document update:** negligible (one Haiku call + embedding for ~10-50 chunks).

### Rate Limiting Strategy

**Anthropic (Haiku) — Context Generation:**
- Rate limit: 4,000 RPM (Tier 1)
- ~11,400 documents → ~3 minutes at 4,000 RPM
- Add 500ms delay between calls for safety → ~95 minutes total
- Bottleneck is likely output generation, not rate limits

**OpenAI — Embedding Generation:**
- Rate limit: 3,000 RPM on Tier 1, 5,000 RPM on Tier 2+
- With batches of 100 and 200ms delay between batches:
  - 5 batches/second × 100 items = 500 embeddings/second
  - 500 × 100 tokens = 50,000 TPM (well under 1M TPM limit)
  - 5 requests/second = 300 RPM (well under 3,000 RPM limit)
- Estimated total time for 295,000 chunks: ~10 minutes

### HNSW Index Parameters

| Parameter        | Value | Rationale                                           |
| ---------------- | ----- | --------------------------------------------------- |
| `m`              | 16    | Connections per node. Default. Good for <1M vectors  |
| `ef_construction`| 64    | Build quality. Default. Higher = better recall, slower build |
| `ef_search`      | 40    | Query-time parameter (default). Can be tuned per-query |

For ~300K vectors, these defaults provide >95% recall@10. If recall needs improvement, increase `ef_search` at query time:

```sql
SET hnsw.ef_search = 100;  -- per-session, higher = better recall
```

[Source: architecture/11-backend-architecture.md#1133-vector-search-pattern]

### Existing Embedding Pattern

The project already generates full-document embeddings for `LegalDocument`:

```prisma
// prisma/schema.prisma:301
embedding Unsupported("vector(1536)")? // Semantic search
```

Look in `lib/sync/ai-summary-queue.ts` for existing OpenAI embedding calls — follow the same pattern for consistency. [Source: lib/sync/ai-summary-queue.ts]

## Testing

**Test location:** `tests/unit/chunks/` [mirroring `lib/chunks/` source structure] [Source: architecture/section-15-summary.md#162-unit-testing]

**Test framework:** Vitest. Mock both Anthropic and OpenAI APIs using `vi.mock()`. [Source: architecture/3-tech-stack.md — Vitest 1.4+]

**Test files:**
- `tests/unit/chunks/generate-context-prefixes.test.ts` (at least 5 tests)
- `tests/unit/chunks/embed-chunks.test.ts` (at least 5 tests)

**Mocking strategy:** Mock SDK constructors and method calls:

```typescript
// Mock Anthropic
vi.mock('@anthropic-ai/sdk', () => ({
  default: vi.fn().mockImplementation(() => ({
    messages: {
      create: vi.fn().mockResolvedValue({
        content: [{ type: 'text', text: '{"prefixes":{"kap1.§1":"Context..."}}' }],
      }),
    },
  })),
}))

// Mock OpenAI
vi.mock('openai', () => ({
  default: vi.fn().mockImplementation(() => ({
    embeddings: {
      create: vi.fn().mockResolvedValue({
        data: [{ embedding: new Array(1536).fill(0.1) }],
        usage: { total_tokens: 100 },
      }),
    },
  })),
}))
```

**Unit tests — context prefix generation:**
1. Normal document: generates prefixes for all chunks, returns correct Map
2. Large document (>200K tokens): splits at division level, makes multiple API calls
3. Response parsing: correctly extracts prefixes from structured JSON response
4. Error handling: retries once on transient failure, throws on persistent
5. Chunks with special characters in paths handled correctly

**Unit tests — embedding function:**
6. `generateEmbedding` combines header + prefix + content → verify OpenAI input string
7. `generateEmbedding` returns 1536-dim vector from API response
8. `generateEmbedding` throws descriptive error when API fails
9. `generateEmbeddingsBatch` sends all items in single API call
10. `generateEmbeddingsBatch` with 100 items → returns 100 embeddings in correct order

**Note on sync tests:** The incremental embedding integration (Task 5) modifies `syncDocumentChunks` — update existing tests in `tests/unit/chunks/sync-document-chunks.test.ts` to mock both Anthropic and OpenAI calls.

**What NOT to test:** The verification script is for manual inspection — no unit tests. HNSW tuning is verified by the existing migration.

## Change Log

| Date       | Version | Description            | Author     |
| ---------- | ------- | ---------------------- | ---------- |
| 2026-02-18 | 1.0     | Initial story creation | Sarah (PO) |
| 2026-02-25 | 2.0     | Add LLM contextual retrieval (Anthropic approach): one Haiku call per document for context prefixes, new `context_prefix` column, large doc splitting strategy, updated cost estimates | PO + Dev |
| 2026-02-25 | 3.0     | Formal story draft: restructured per story template, added source references, previous story insights, complete dev notes with all architecture context | Bob (SM) |
| 2026-02-25 | 3.1     | PO validation fixes: added bulk chunking prerequisite note, clarified Prisma column naming, referenced actual Anthropic SDK location (`lib/sfs/amendment-llm-prompt.ts`), added `--skip-context` rationale, standardized embedding input assembly format | Sarah (PO) |

## Dev Agent Record

### Agent Model Used

Claude Opus 4.6

### Debug Log References

- Prisma generate EPERM: `query_engine-windows.dll.node` locked by running dev server — used `prisma db push` + manual migration + `prisma migrate resolve --applied` pattern (same as Story 14.2)
- Test mock issue: `vi.mock` with `default: vi.fn().mockImplementation(...)` not constructable — resolved by using `setAnthropicClient`/`setOpenAIClient` DI pattern instead
- Large doc split test: initial test data (600K chars = 150K tokens) below 200K threshold — fixed by increasing to 1M chars

### Completion Notes List

- All 8 tasks complete, 57 chunk unit tests pass (12 embed + 7 context + 7 sync + 31 chunk-document)
- `npx tsc --noEmit` passes with zero errors
- Full regression: 3171 passed, 14 failed (all pre-existing integration/auth/breadcrumb failures)
- Prisma client regeneration blocked by file lock — requires stopping dev server, then `npx prisma generate`
- SyncResult interface gained `chunksEmbedded` field — no external consumers affected (only used in bulk-sync-chunks.ts which destructures)

### File List

- `prisma/schema.prisma` — MODIFIED (added `context_prefix` to ContentChunk)
- `prisma/migrations/20260225200000_add_context_prefix_to_content_chunks/migration.sql` — NEW
- `lib/chunks/generate-context-prefixes.ts` — NEW (LLM context prefix generation)
- `lib/chunks/embed-chunks.ts` — NEW (OpenAI embedding generation + batch)
- `lib/chunks/sync-document-chunks.ts` — MODIFIED (incremental context + embedding after chunk creation)
- `lib/chunks/index.ts` — MODIFIED (added new exports)
- `scripts/generate-embeddings.ts` — NEW (batch script with --estimate, --resume, --limit, --skip-context, --source-type)
- `scripts/verify-embeddings.ts` — NEW (verification script with Swedish test queries)
- `tests/unit/chunks/generate-context-prefixes.test.ts` — NEW (7 tests)
- `tests/unit/chunks/embed-chunks.test.ts` — NEW (12 tests)
- `tests/unit/chunks/sync-document-chunks.test.ts` — MODIFIED (added 2 tests for incremental embedding)

### Change Log

| Date | Description |
|---|---|
| 2026-02-25 | Implemented all 8 tasks: schema migration, context prefix generation (Haiku), embedding function (OpenAI), batch script, incremental sync, HNSW verification, verification script, 19 new unit tests |

## QA Results
