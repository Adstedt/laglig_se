# Story 14.3: Embedding Generation Pipeline

## Status

Draft

## Story

**As a** platform preparing for semantic search,
**I want** embeddings generated for all content chunks across the database,
**so that** the RAG pipeline can perform vector similarity search.

## Context & Dependencies

**Why this story exists:**

Story 14.2 creates the `ContentChunk` model and populates it with paragraph-level chunks derived from `json_content`. But those chunks have `embedding = NULL` — they cannot be searched by vector similarity. This story builds the embedding generation pipeline: a function that calls OpenAI's `text-embedding-3-small` to generate 1536-dim vectors for each chunk, a batch script for initial full-DB generation, and incremental embedding for new chunks. After this story, the database is ready for vector similarity search.

**Builds on:**

- Story 14.2 (ContentChunk Model & Chunking Pipeline) — provides `ContentChunk` records with `content`, `contextualHeader`, and `embedding = NULL`
- `ContentChunk.embedding` — `Unsupported("vector(1536)")` field, HNSW index already created by Story 14.2
- OpenAI SDK — already in `package.json` (used for existing AI features like `ai-summary-queue.ts`)
- `LegalDocument.embedding` — existing full-document embedding using same model [Source: prisma/schema.prisma:301]
- Embedding model confirmed: OpenAI `text-embedding-3-small` (1536 dimensions)
- Token count available on each chunk (`ContentChunk.token_count`) from Story 14.2

**Depends on:** Story 14.2 must be complete (ContentChunk table must exist with HNSW index and populated chunks).

**Depended on by:**

- All RAG retrieval stories — vector similarity queries require embeddings to exist
- Story 14.4 (Retrieval API) or similar — queries ContentChunk with `<=>` cosine distance operator
- Future incremental sync stories — new documents go through chunk + embed pipeline

## Acceptance Criteria

### Embedding Function

1. Embedding model: OpenAI `text-embedding-3-small` (1536 dimensions) — confirmed in architecture
2. Embedding function in `lib/chunks/embed-chunks.ts` takes chunk text + contextual header and returns a 1536-dim embedding vector
3. Combines `contextualHeader + "\n\n" + content` for embedding input (contextual retrieval pattern — the header provides document context that improves retrieval precision)

### Batch Script

4. Batch embedding script `scripts/generate-embeddings.ts` for initial full-DB generation
5. Script has cost estimation mode (`--estimate` flag) — calculates total tokens across all unembedded chunks and estimated cost before running
6. Script processes chunks in batches of 100 (OpenAI embedding API batch limit per request), with rate limiting between batches
7. Script has progress logging: `"Embedded 5,000/150,000 chunks (3.3%) | Cost so far: $0.10"`
8. Script has resume-on-failure: tracks last processed chunk ID in a cursor file, can restart from where it left off with `--resume` flag

### Incremental Embedding

9. Incremental embedding: when new chunks are created via `syncDocumentChunks` (Story 14.2), embeddings are generated immediately after chunk creation (synchronous for now)

### Index Tuning & Verification

10. pgvector HNSW index parameters tuned for expected dataset size (~150K-300K chunks): `ef_construction = 64`, `m = 16`
11. Verification script `scripts/verify-embeddings.ts` runs sample similarity queries and displays top-5 results with similarity scores for manual inspection

### Testing

12. At least 8 unit tests (mock OpenAI API)

## Tasks / Subtasks

- [ ] **Task 1: Embedding function** (AC: 1-3)
  - [ ] Create `lib/chunks/embed-chunks.ts`
  - [ ] Implement `generateEmbedding(text: string, contextualHeader: string): Promise<number[]>`
    - Combine input: `const input = contextualHeader + "\n\n" + text`
    - Call OpenAI API: `openai.embeddings.create({ model: 'text-embedding-3-small', input })`
    - Return the embedding vector (`response.data[0].embedding`)
    - Handle errors: wrap in try/catch, throw descriptive error with chunk context
  - [ ] Implement `generateEmbeddingsBatch(items: Array<{ text: string, contextualHeader: string }>): Promise<number[][]>`
    - Combine inputs for all items
    - Call OpenAI API with array input (batch mode): `openai.embeddings.create({ model: 'text-embedding-3-small', input: inputs })`
    - Return array of embedding vectors in same order
    - Maximum 100 items per batch call (OpenAI limit)
  - [ ] Set up OpenAI client — check for existing client setup in `lib/` or create `lib/chunks/openai-client.ts`
  - [ ] Export both functions for use by batch script and incremental sync

- [ ] **Task 2: Batch embedding script** (AC: 4-8)
  - [ ] Create `scripts/generate-embeddings.ts`
  - [ ] Accept CLI flags:
    - `--estimate` — calculate total tokens and cost, then exit (no API calls)
    - `--limit N` — process only first N unembedded chunks
    - `--resume` — read cursor from `data/embedding-progress.json`, start from last processed chunk
    - `--batch-size N` — override default batch size of 100
    - `--source-type LEGAL_DOCUMENT` — filter by source type
  - [ ] Query unembedded chunks: `WHERE embedding IS NULL ORDER BY id ASC`
    - Note: Cannot use Prisma `where` on `Unsupported` type — use `$queryRaw` to find chunks without embeddings
  - [ ] Cost estimation mode (`--estimate`):
    - Sum `token_count` across all unembedded chunks
    - Calculate cost: `totalTokens / 1_000_000 * 0.02` (text-embedding-3-small pricing: $0.02/1M tokens)
    - Print: `"Unembedded chunks: 295,000 | Total tokens: ~29,500,000 | Estimated cost: $0.59"`
    - Exit without making API calls
  - [ ] Batch processing loop:
    - Fetch next batch of 100 unembedded chunks (by ID cursor)
    - Call `generateEmbeddingsBatch()` for the batch
    - Write embeddings to DB using `$executeRaw` for each chunk:
      ```sql
      UPDATE content_chunks SET embedding = $1::vector WHERE id = $2
      ```
    - Update cursor file: write last processed chunk ID to `data/embedding-progress.json`
    - Rate limiting: `await sleep(200)` between batches (300 RPM = 5 per second, 200ms gap is conservative)
  - [ ] Progress logging every 10 batches (1,000 chunks):
    ```
    [Embeddings] 5,000/295,000 (1.7%) | Batch 50 | Cost: $0.10 | Elapsed: 2m 15s
    ```
  - [ ] Error handling per batch: if a batch fails, log error, save cursor, and continue with next batch (or abort after 3 consecutive failures)
  - [ ] Summary report at end: total embedded, errors, cost, duration

- [ ] **Task 3: Incremental embedding** (AC: 9)
  - [ ] Modify `syncDocumentChunks` in `lib/chunks/sync-document-chunks.ts` (from Story 14.2) or create a wrapper `syncDocumentChunksWithEmbeddings`
  - [ ] After creating new chunks, call `generateEmbeddingsBatch()` for all new chunks
  - [ ] Write embeddings to DB using `$executeRaw`
  - [ ] This is synchronous (blocking) — acceptable for now since document syncs are infrequent and chunks per document are small (~10-50)
  - [ ] If embedding fails, log error but do not roll back chunk creation (chunks without embeddings can be retried by batch script)

- [ ] **Task 4: HNSW index tuning** (AC: 10)
  - [ ] Verify HNSW index parameters from Story 14.2 migration: `m = 16`, `ef_construction = 64`
  - [ ] These parameters are appropriate for ~300K vectors:
    - `m = 16`: balanced between recall and memory (default is 16)
    - `ef_construction = 64`: build-time quality (default is 64)
  - [ ] If Story 14.2 used different parameters, create a new migration to alter the index
  - [ ] Document the parameter choices and their trade-offs in code comments
  - [ ] Note: Query-time `ef_search` parameter can be set per-query via `SET hnsw.ef_search = 100` for tuning recall vs. speed

- [ ] **Task 5: Verification script** (AC: 11)
  - [ ] Create `scripts/verify-embeddings.ts`
  - [ ] Define 5-10 test queries in Swedish covering different legal domains:
    ```typescript
    const testQueries = [
      "arbetsgivarens skyldigheter for skyddsutrustning",      // employer PPE obligations
      "semesterersattning vid uppsagning",                      // vacation pay on termination
      "krav pa ventilation i arbetslokaler",                    // ventilation requirements
      "anmalan av allvarligt olycksfall",                       // serious accident reporting
      "diskrimineringsforbudet vid anstallning",                // discrimination in hiring
    ]
    ```
  - [ ] For each query:
    - Generate query embedding via `generateEmbedding(query, "")`
    - Run cosine similarity search via `$queryRaw`:
      ```sql
      SELECT id, source_id, path, contextual_header, content,
        1 - (embedding <=> $1::vector) as similarity
      FROM content_chunks
      WHERE embedding IS NOT NULL
      ORDER BY embedding <=> $1::vector
      LIMIT 5
      ```
    - Print formatted results with similarity scores
  - [ ] Output format:
    ```
    Query: "arbetsgivarens skyldigheter for skyddsutrustning"
    ─────────────────────────────────────────
    1. [0.892] Arbetsmiljolagen (SFS 1977:1160) > Kap 2 > 7 §
       "Arbetsgivaren skall vidta alla atgarder som behovs for att forebygga..."
    2. [0.856] AFS 2001:1 Systematiskt arbetsmiljoarbete > Kap 3 > 4 §
       "Arbetsgivaren ska se till att de arbetstagare som..."
    ...
    ```
  - [ ] No assertions — this is for human inspection of retrieval quality

- [ ] **Task 6: Tests** (AC: 12)
  - [ ] Create `tests/unit/chunks/embed-chunks.test.ts`
  - [ ] Write at least 8 unit tests (see Testing section for details)
  - [ ] Verify `npx tsc --noEmit` passes
  - [ ] Verify all existing tests still pass

## Dev Notes

### Source Tree

```
lib/chunks/
  ├── chunk-document.ts                  — FROM Story 14.2
  ├── sync-document-chunks.ts            — FROM Story 14.2 — MODIFIED (add incremental embedding)
  ├── token-count.ts                     — FROM Story 14.2
  ├── embed-chunks.ts                    — NEW — generateEmbedding(), generateEmbeddingsBatch()
  ├── openai-client.ts                   — NEW (if no existing client) — OpenAI SDK singleton
  └── index.ts                           — MODIFIED — add embedding exports

scripts/
  ├── generate-embeddings.ts             — NEW — batch embedding script
  └── verify-embeddings.ts               — NEW — verification / sanity check script

data/
  └── embedding-progress.json            — NEW — resume cursor (gitignored)
```

### OpenAI SDK Setup

The OpenAI SDK is already in `package.json`. Check for existing client initialization:

```typescript
// If no existing client, create lib/chunks/openai-client.ts:
import OpenAI from 'openai'

export const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
})
```

If an OpenAI client already exists elsewhere in the codebase (e.g., `lib/openai.ts` or in `lib/sync/ai-summary-queue.ts`), reuse it rather than creating a duplicate.

### Embedding API Details

**Model:** `text-embedding-3-small`
**Dimensions:** 1536
**Pricing:** $0.02 per 1M tokens (very cheap)
**Rate limit:** 3,000 RPM on Tier 1, 5,000 RPM on Tier 2+
**Max tokens per request:** 8,191 tokens per input
**Batch size:** Up to 2,048 inputs per request (we use 100 for safety and memory)

**API call:**
```typescript
const response = await openai.embeddings.create({
  model: 'text-embedding-3-small',
  input: ['text1', 'text2', ...],  // batch of strings
})
// response.data[i].embedding is number[] of length 1536
```

### Contextual Retrieval Pattern

The key insight for high-quality retrieval: prepend the contextual header to the chunk text before embedding. This means the embedding captures both the content and its position in the document hierarchy.

```typescript
// Embedding input for a chunk:
const embeddingInput = `${contextualHeader}\n\n${content}`

// Example:
// "Arbetsmiljolagen (SFS 1977:1160) > Kap 2: Arbetsmiljons beskaffenhet > 7 §\n\n
//  Arbetsgivaren skall vidta alla atgarder som behovs for att forebygga att arbetstagaren
//  utsatts for ohalsa eller olycksfall..."
```

This is based on Anthropic's contextual retrieval research — adding document context to chunks before embedding significantly improves retrieval accuracy.

### Writing Embeddings with Prisma

Since `embedding` uses `Unsupported("vector(1536)")`, Prisma ORM cannot read/write it directly. Use raw SQL:

```typescript
// Write embedding for a single chunk
await prisma.$executeRaw`
  UPDATE content_chunks
  SET embedding = ${vectorArrayToString(embedding)}::vector
  WHERE id = ${chunkId}
`

// Helper to convert number[] to pgvector string format
function vectorArrayToString(vector: number[]): string {
  return `[${vector.join(',')}]`
}

// Query with cosine similarity
const results = await prisma.$queryRaw`
  SELECT id, content, contextual_header,
    1 - (embedding <=> ${vectorArrayToString(queryEmbedding)}::vector) as similarity
  FROM content_chunks
  WHERE embedding IS NOT NULL
    AND source_type = 'LEGAL_DOCUMENT'
  ORDER BY embedding <=> ${vectorArrayToString(queryEmbedding)}::vector
  LIMIT ${limit}
`
```

**Important:** The `<=>` operator is cosine distance (not similarity). `similarity = 1 - distance`. Higher similarity = more relevant.

### Resume Cursor Pattern

The batch script tracks progress in a JSON file to support resume-on-failure:

```typescript
// data/embedding-progress.json
{
  "lastProcessedChunkId": "clxyz123abc",
  "totalProcessed": 5000,
  "totalCost": 0.10,
  "lastRunAt": "2026-02-18T14:30:00Z"
}
```

On `--resume`, the script reads this file and adds `WHERE id > ${lastProcessedChunkId}` to the query. This works because chunks are ordered by `id ASC` (cuid IDs are roughly time-ordered).

### Cost Estimation

Expected scale (from Story 14.2):
- ~295,000 chunks total
- Average ~100 tokens per chunk (contextual header + content)
- Total: ~29.5M tokens
- Cost: 29.5M / 1M * $0.02 = **$0.59**

With contextual headers (~50 tokens average), total input is ~44M tokens = **$0.88**

This is extremely affordable for a one-time batch operation.

### Rate Limiting Strategy

OpenAI's rate limits for `text-embedding-3-small`:
- Tier 1: 3,000 RPM, 1,000,000 TPM
- Tier 2+: 5,000 RPM, 1,000,000 TPM

With batches of 100 and 200ms delay between batches:
- 5 batches/second * 100 items = 500 embeddings/second
- 500 * 100 tokens = 50,000 TPM (well under 1M TPM limit)
- 5 requests/second = 300 RPM (well under 3,000 RPM limit)

Estimated total time for 295,000 chunks: ~10 minutes.

### HNSW Index Parameters

| Parameter        | Value | Rationale                                           |
| ---------------- | ----- | --------------------------------------------------- |
| `m`              | 16    | Connections per node. Default. Good for <1M vectors  |
| `ef_construction`| 64    | Build quality. Default. Higher = better recall, slower build |
| `ef_search`      | 40    | Query-time parameter (default). Can be tuned per-query |

For ~300K vectors, these defaults provide >95% recall@10. If recall needs improvement, increase `ef_search` at query time:

```sql
SET hnsw.ef_search = 100;  -- per-session, higher = better recall
```

### Existing Embedding Pattern

The project already generates full-document embeddings for `LegalDocument`:

```prisma
// prisma/schema.prisma:301
embedding Unsupported("vector(1536)")? // Semantic search
```

Look in `lib/sync/ai-summary-queue.ts` for existing OpenAI embedding calls — follow the same pattern for consistency. [Source: lib/sync/ai-summary-queue.ts]

## Testing

**Test location:** `tests/unit/chunks/` [mirroring `lib/chunks/` source structure]

**Test framework:** Vitest. Mock OpenAI API using `vi.mock('openai')`.

**Test file:** `tests/unit/chunks/embed-chunks.test.ts` (at least 8 tests)

**Mocking strategy:** Mock the OpenAI SDK constructor and `embeddings.create` method:

```typescript
vi.mock('openai', () => ({
  default: vi.fn().mockImplementation(() => ({
    embeddings: {
      create: vi.fn().mockResolvedValue({
        data: [{ embedding: new Array(1536).fill(0.1) }],
        usage: { total_tokens: 100 },
      }),
    },
  })),
}))
```

**Unit tests — embedding function:**
1. `generateEmbedding` combines contextualHeader + content with separator → verify OpenAI input string
2. `generateEmbedding` returns 1536-dim vector from API response
3. `generateEmbedding` throws descriptive error when API fails (include chunk context in error message)
4. `generateEmbeddingsBatch` sends all items in single API call → verify input array
5. `generateEmbeddingsBatch` with 100 items → returns 100 embeddings in correct order

**Unit tests — batch logic:**
6. Cost estimation: 1,000 chunks with total `token_count` of 100,000 → estimated cost = $0.002
7. Resume logic: given cursor file with `lastProcessedChunkId`, verify query starts from that ID
8. Batch processing: 250 chunks with batch size 100 → produces 3 API calls (100 + 100 + 50)

**Note on sync tests:** The incremental embedding integration (Task 3) modifies `syncDocumentChunks` — update existing tests in `tests/unit/chunks/sync-document-chunks.test.ts` to mock the embedding call, or add new tests for the embedding wrapper.

**What NOT to test:** The verification script (`scripts/verify-embeddings.ts`) is a manual inspection tool — no unit tests needed. The HNSW index tuning (Task 4) is a migration change — verified by running the migration, not unit tests.

## Change Log

| Date       | Version | Description            | Author     |
| ---------- | ------- | ---------------------- | ---------- |
| 2026-02-18 | 1.0     | Initial story creation | Sarah (PO) |

## Dev Agent Record

### Agent Model Used

### Debug Log References

### Completion Notes List

### File List

## QA Results
