# Story 14.8: RAG Retrieval Pipeline

## Status

Draft

## Story

**As a** compliance partner agent,
**I want** to retrieve the most relevant context from all knowledge sources in a single query,
**so that** my responses are grounded in accurate, company-specific information.

## Context & Dependencies

The retrieval pipeline is the core of the agent's intelligence. It orchestrates a two-stage retrieval process — vector similarity search followed by Cohere Rerank v4 cross-encoder reranking — then assembles context from multiple sources (laws, files, assessments) into a token-budgeted prompt block. Workspace scoping is non-negotiable — never return another workspace's private data.

**Key existing infrastructure (from completed stories):**

- **ContentChunk table** (Story 14.2): 228,778 chunks with contextual headers, paragraph-level granularity, pgvector HNSW index [Source: architecture/chunking-strategy.md]
- **Embeddings** (Story 14.3): All chunks embedded with OpenAI `text-embedding-3-small` (1536 dims), contextual headers + LLM context prefixes included in embedding input [Source: lib/chunks/embed-chunks.ts]
- **Cohere Rerank v4** (Story 14.3): Cross-encoder reranking module at `lib/search/rerank.ts` — benchmarked at +38% avg relevance improvement (0.634 → 0.874 on 55 queries). Graceful degradation when API key missing or API fails [Source: lib/search/rerank.ts]
- **Tool definitions** (Story 14.7): `search_laws` and `search_company_files` tools define the agent-facing interface — this story implements the retrieval logic those tools call

**Depends on:** 14.2 (ContentChunk model), 14.3 (embeddings + rerank module), 14.7 (tool definitions reference retrieval)

**Depended on by:** 14.9 (system prompt references retrieval behavior), 14.10 + 14.11 (UX surfaces retrieved context with citations)

**Scope note — query expansion:** The epic mentions "query expansion: if initial query returns low-relevance results, agent can reformulate and retry." This is an agent-level behavior (the LLM decides to reformulate and call the retrieval tool again), not a retrieval pipeline feature. It is handled in Story 14.9 (system prompt behavioral design) where the agent is instructed to reformulate queries when results are insufficient.

## Acceptance Criteria

### Core Retrieval

1. Unified retrieval function `retrieveContext(query, workspaceId, options)` in `lib/agent/retrieval.ts`. Options include `topK`, `sourceType`, `contentType`, `overFetchMultiplier`, and `minRelevanceScore` (optional — default: no filtering; prepares for future confidence thresholds per [Source: architecture/chunking-strategy.md#Confidence-Thresholds])
2. Two-stage retrieval: (a) pgvector HNSW cosine similarity search for top-K candidates, then (b) Cohere Rerank v4 cross-encoder to reorder and trim to final result set [Source: architecture/chunking-strategy.md#Two-Stage-Pipeline]
3. Over-fetch pattern: when reranking is enabled, vector search fetches `4 × topK` candidates (e.g., 20 candidates to return top 5) [Source: architecture/chunking-strategy.md#Vector-Search]
4. Workspace scoping: `WHERE (workspace_id IS NULL OR workspace_id = $workspaceId)` — never returns other workspace's private chunks
5. Returns chunks ranked by relevance score with metadata (sourceType, documentNumber, contextualHeader, path, relevanceScore)
6. Supports filtering by sourceType (e.g., only `LEGAL_DOCUMENT`, only `USER_FILE`)
7. Supports filtering by contentType (e.g., only `SFS_LAW`, only `AGENCY_REGULATION`) via join to LegalDocument

### Graceful Degradation

8. If Cohere API key is missing or API fails, the pipeline returns vector-search results unchanged (rerank module already handles this — wire it through) [Source: lib/search/rerank.ts]
9. If embedding API fails, return descriptive error — do not return empty results silently

### Legal Reference Detection

10. Legal reference detector `detectLegalReferences(query)` in `lib/agent/legal-ref-detector.ts` — extracts SFS numbers and chapter/section references from natural language queries
11. Detect SFS numbers: `"SFS 1977:1160"`, `"1977:1160"` — returns structured `{ sfsNumber: string }[]`
12. Detect chapter/section references: `"3 kap. 5 §"`, `"5 §"` — returns structured `{ chapter?: number, section: number }[]`
13. When legal references detected, add them as supplementary metadata to retrieval results (not as a separate keyword search — BM25 deferred to Story 14.13) [Source: architecture/chunking-strategy.md#Hybrid-Search-Deferred]

### Context Assembly

14. Context assembly function `assembleAgentContext(chunks, options)` in `lib/agent/context-assembly.ts` — formats retrieved chunks into a prompt-ready context block
15. Token budget management: configurable max tokens (default 8000), prioritize highest-relevance chunks, use `token_count` from ContentChunk metadata
16. Deduplication: if multiple chunks from the same document are retrieved, group them and sort by path (preserve document order within document)
17. Source attribution: each chunk in the assembled context includes its source reference for citation (document title, number, chapter/section)
18. Output format uses Swedish labels:
    ```
    --- Källa: Arbetsmiljölagen (SFS 1977:1160) > Kap 2 > 3 § ---
    Arbetsförhållandena skall anpassas till människans förutsättningar...

    --- Källa: Företagets arbetsmiljöpolicy.pdf > Sida 3 ---
    Vår policy för systematiskt arbetsmiljöarbete inkluderar...
    ```

### Performance

19. End-to-end retrieval latency < 800ms for top-5 results (includes embedding + vector search ~50ms + rerank ~420ms + assembly) [Source: Story 14.3 benchmark data]
20. Query embedding reuse: if the same query string is used in the same request context, reuse the embedding (avoid redundant OpenAI calls)

### Testing

21. At least 12 unit tests covering: retrieval function, workspace scoping, rerank wiring, context assembly, token budget, legal reference detection

## Tasks / Subtasks

- [ ] **Task 1: Core retrieval function** (AC: 1-9)
  - [ ] Create `lib/agent/retrieval.ts` with `retrieveContext()` function
  - [ ] Implement query embedding via `generateEmbedding()` from `lib/chunks/embed-chunks.ts` — use `buildEmbeddingInput(query, '', '')` for query embedding (no context prefix for queries) [Source: lib/chunks/embed-chunks.ts]
  - [ ] Implement pgvector cosine similarity search via `prisma.$queryRaw` with parameterized templates (same pattern as Story 14.2/14.3)
  - [ ] Wire in `rerank()` from `lib/search/rerank.ts` as second stage — build rerank text via `buildRerankText(content, contextPrefix, contextualHeader)` for each candidate [Source: lib/search/rerank.ts]
  - [ ] Implement over-fetch: when reranking enabled, fetch `overFetchMultiplier × topK` candidates (default 4×)
  - [ ] Add workspace scoping to all queries: `WHERE (workspace_id IS NULL OR workspace_id = $workspaceId)`
  - [ ] Add sourceType and contentType filtering options
  - [ ] Add optional `minRelevanceScore` filter — when set, exclude results below threshold after reranking (default: no filtering). This prepares for future confidence thresholds without requiring a separate story
  - [ ] Return `RetrievalResult[]` with fields: `id, content, contextualHeader, contextPrefix, path, sourceType, sourceId, documentNumber, similarity, relevanceScore, metadata`
  - [ ] Handle graceful degradation: rerank passthrough when `reranked: false` — return vector-search results with `relevanceScore: 0` (AC 8)
  - [ ] Handle embedding API failure: catch error from `generateEmbedding()`, throw descriptive `RetrievalError` with query context — never return empty results silently (AC 9)

- [ ] **Task 2: Legal reference detection** (AC: 10-13)
  - [ ] Create `lib/agent/legal-ref-detector.ts` with `detectLegalReferences(query)` function
  - [ ] Implement SFS number regex: `/(?:SFS\s+)?(\d{4}:\d+)/g`
  - [ ] Implement chapter/section regex: `/(\d+)\s*kap\.\s*(\d+[a-z]?)\s*§/g` and standalone `/(\d+[a-z]?)\s*§/g`
  - [ ] Return typed result: `{ sfsNumbers: string[], sectionRefs: Array<{ chapter?: number, section: string }> }`
  - [ ] Attach detected references as metadata on retrieval results (enrich, not filter)
  - [ ] Note: BM25 keyword search is explicitly deferred to Story 14.13 — do NOT implement full-text search here [Source: architecture/chunking-strategy.md#Hybrid-Search-Deferred]

- [ ] **Task 3: Context assembly** (AC: 14-18)
  - [ ] Create `lib/agent/context-assembly.ts` with `assembleAgentContext()` function
  - [ ] Implement token budget management — use `token_count` field from ContentChunk, configurable max (default 8000)
  - [ ] Implement deduplication: group chunks from same source document, sort by path within group (preserve document order)
  - [ ] Format each chunk with Swedish source label: `--- Källa: {contextualHeader} ---\n{content}`
  - [ ] Separate chunks with blank lines for readability
  - [ ] Return assembled context string + metadata array (sources used, total tokens, chunks included/excluded)

- [ ] **Task 4: Performance & caching** (AC: 19-20)
  - [ ] Add structured timing instrumentation to `retrieveContext()` — log each stage as `{ stage: 'embed' | 'vectorSearch' | 'rerank' | 'assembly', latencyMs: number, itemsIn?: number, itemsOut?: number }` and return aggregate `timings` object alongside results
  - [ ] Implement query embedding cache: simple `Map<string, number[]>` scoped to request lifecycle (passed as optional arg or using a cache wrapper)
  - [ ] Verify end-to-end latency target via logged timings
  - [ ] Set pgvector `hnsw.ef_search` within the query transaction for tunable recall [Source: lib/chunks/embed-chunks.ts#HNSW-comments]

- [ ] **Task 5: Tests** (AC: 21)
  - [ ] Create `tests/unit/agent/retrieval.test.ts` — mock `prisma.$queryRaw`, mock `generateEmbedding`, mock `rerank()`
    - Test: vector search + rerank pipeline wiring (over-fetch, rerank call, final result order)
    - Test: workspace scoping (verify WHERE clause includes workspace filter)
    - Test: graceful degradation (rerank returns `reranked: false` → results still returned)
    - Test: sourceType and contentType filtering
  - [ ] Create `tests/unit/agent/context-assembly.test.ts`
    - Test: token budget enforcement (20 chunks × 500 tokens each should trim to fit 8000 budget)
    - Test: deduplication (5 chunks from same doc grouped and ordered by path)
    - Test: output format matches Swedish label spec
    - Test: empty input returns empty context
  - [ ] Create `tests/unit/agent/legal-ref-detector.test.ts`
    - Test: SFS number detection (`"SFS 1977:1160"`, `"1977:1160"`, multiple in one string)
    - Test: chapter/section detection (`"3 kap. 5 §"`, `"5 §"`, `"12a §"`)
    - Test: no references → empty result
    - Test: mixed references in natural language sentence
  - [ ] Verify no cross-workspace data leakage in retrieval tests
  - [ ] Verify `npx tsc --noEmit` passes
  - [ ] Verify all existing tests still pass (3171+ tests)

## Dev Notes

### Previous Story Insights

**From Story 14.3 (Embedding Generation Pipeline — Done):**

- **Prisma `Unsupported` type:** Cannot use Prisma ORM to read/write `embedding` or query by it. Must use `$executeRaw` / `$queryRaw` for all vector operations. [Source: Story 14.3 dev notes]
- **Cohere Rerank benchmark results:** Avg relevance improved from 0.634 → 0.874 (+38%) on 55 queries. Scenario-based queries (natural language, vocabulary mismatch) showed +46% improvement. Avg rerank latency: 420ms. [Source: Story 14.3 dev agent record]
- **BM25 hybrid search deferred:** Decision made after rerank benchmark — reranker already bridges vocabulary gap. BM25 deferred until Story 14.13 ground truth labeling quantifies failure modes. [Source: architecture/chunking-strategy.md#Decision-Log]
- **Database state:** 228,778 chunks, 194,652 with context prefixes (85.1%), 100% with embeddings, HNSW index active (m=16, ef_construction=64). Search latency ~300ms including network. [Source: Story 14.3 operational notes]

**From Story 14.7 (Agent Tool Definitions — Draft):**

- `search_laws` tool expects: `query (string), contentType (optional), limit (default 10)`. Returns: passages with `contextualHeader, documentNumber, similarity`. [Source: Story 14.7 AC#1]
- `search_company_files` tool expects: `query (string), limit (default 5)`. Filters by `source_type = 'USER_FILE' AND workspace_id = $workspaceId`. [Source: Story 14.7 AC#5]
- Tool responses include `_meta: { tool, executionTime, chunksSearched }` for transparency. [Source: Story 14.7 AC#14]
- The retrieval function in this story is the **implementation behind** both `search_laws` and `search_company_files` tools.

### Existing Rerank Module

The rerank module is **already implemented and tested** (26 tests) — this story wires it into the retrieval pipeline:

```typescript
// lib/search/rerank.ts — USE AS-IS
import { rerank, buildRerankText } from '@/lib/search'
import type { RerankableDocument, RerankResult } from '@/lib/search'

// Build text for each candidate chunk:
const rerankDocs = candidates.map(chunk => ({
  ...chunk,
  text: buildRerankText(chunk.content, chunk.context_prefix, chunk.contextual_header)
}))

// Rerank — returns documents sorted by relevanceScore
const { results, reranked, latencyMs } = await rerank(query, rerankDocs, {
  model: 'rerank-v4.0-pro',  // default
  topN: topK,                 // final result count
})
```

[Source: lib/search/rerank.ts, lib/search/index.ts]

### Existing Embedding Module

Query embedding uses the same function as document embedding:

```typescript
// lib/chunks/embed-chunks.ts — USE AS-IS
import { generateEmbedding, vectorToString } from '@/lib/chunks/embed-chunks'

// For query embedding, contextPrefix is empty (queries don't have LLM prefixes)
const { embedding } = await generateEmbedding(query, '', '')
const queryVectorStr = vectorToString(embedding)
```

[Source: lib/chunks/embed-chunks.ts]

### pgvector Query Pattern

```sql
-- Two-stage retrieval: Stage 1 (vector search with over-fetch)
SELECT
  cc.id, cc.content, cc.contextual_header, cc.context_prefix,
  cc.path, cc.source_type, cc.source_id, cc.token_count, cc.metadata,
  1 - (cc.embedding <=> $1::vector) AS similarity
FROM content_chunks cc
LEFT JOIN legal_documents ld
  ON cc.source_type = 'LEGAL_DOCUMENT' AND cc.source_id = ld.id::text
WHERE cc.embedding IS NOT NULL
  AND (cc.workspace_id IS NULL OR cc.workspace_id = $2)
  AND ($3::text IS NULL OR cc.source_type = $3)
  AND ($4::text IS NULL OR ld.content_type = $4)
ORDER BY cc.embedding <=> $1::vector ASC
LIMIT $5
```

**Important:** The `<=>` operator is cosine **distance** (not similarity). `similarity = 1 - distance`. Order by distance ASC for most similar first. Use `prisma.$queryRaw` with parameterized templates. [Source: architecture/11-backend-architecture.md#1133-vector-search-pattern, Story 14.3 dev notes]

### Source Tree

```
lib/agent/
  ├── retrieval.ts              — NEW — core retrieveContext() function
  ├── context-assembly.ts       — NEW — assembleAgentContext() function
  └── legal-ref-detector.ts     — NEW — detectLegalReferences() function

lib/search/
  ├── rerank.ts                 — EXISTING (from 14.3) — rerank(), buildRerankText()
  └── index.ts                  — EXISTING (from 14.3) — barrel exports

lib/chunks/
  ├── embed-chunks.ts           — EXISTING (from 14.3) — generateEmbedding(), vectorToString()
  └── index.ts                  — EXISTING (from 14.3)

tests/unit/agent/
  ├── retrieval.test.ts         — NEW
  ├── context-assembly.test.ts  — NEW
  └── legal-ref-detector.test.ts — NEW
```

[Source: architecture/12-unified-project-structure.md — lib/ for core business logic, tests/unit/ mirroring lib/ structure]

### Legal Reference Regex Patterns

```typescript
// SFS number: "SFS 1977:1160" or just "1977:1160"
const SFS_PATTERN = /(?:SFS\s+)?(\d{4}:\d+)/g

// Chapter + section: "3 kap. 5 §"
const CHAPTER_SECTION_PATTERN = /(\d+)\s*kap\.\s*(\d+[a-z]?)\s*§/g

// Standalone section: "5 §" (only when not preceded by "kap.")
const SECTION_PATTERN = /(?<!\bkap\.\s*)(\d+[a-z]?)\s*§/g
```

[Source: architecture/chunking-strategy.md — path reference table uses `kap{N}.§{M}` format]

### Context Assembly Output Format

```
--- Källa: Arbetsmiljölagen (SFS 1977:1160) > Kap 2: Arbetsmiljöns beskaffenhet > 3 § ---
Arbetsförhållandena skall anpassas till människans förutsättningar...

--- Källa: Företagets arbetsmiljöpolicy.pdf > Sida 3 ---
Vår policy för systematiskt arbetsmiljöarbete inkluderar...
```

- Use `contextualHeader` directly as the label (already formatted as breadcrumb in Story 14.2)
- For USER_FILE chunks, use filename + page/section if available from metadata
- Each chunk separated by blank line

### Performance Budget Breakdown

| Stage | Expected Latency | Notes |
|---|---|---|
| Query embedding (OpenAI) | ~100ms | Single text-embedding-3-small call |
| Vector search (pgvector HNSW) | ~50ms | Top-20, includes Supabase network |
| Rerank (Cohere v4) | ~420ms | 20 docs, benchmarked avg |
| Context assembly | ~5ms | In-memory string formatting |
| **Total** | **~575ms** | Well within 800ms budget |

[Source: Story 14.3 benchmark data, architecture/chunking-strategy.md#Benchmark-Results]

## Testing

**Test location:** `tests/unit/agent/` [mirroring `lib/agent/` source structure] [Source: architecture/12-unified-project-structure.md]

**Test framework:** Vitest with `vi.mock()`. Mock Prisma `$queryRaw`, OpenAI embeddings API, and Cohere rerank. [Source: architecture/3-tech-stack.md — Vitest 1.4+]

**Mocking strategy:**

```typescript
// Mock Prisma
vi.mock('@/lib/prisma', () => ({
  prisma: { $queryRaw: vi.fn() }
}))

// Mock embedding
vi.mock('@/lib/chunks/embed-chunks', () => ({
  generateEmbedding: vi.fn().mockResolvedValue({
    embedding: new Array(1536).fill(0.1),
    tokensUsed: 10
  }),
  vectorToString: vi.fn((v: number[]) => `[${v.join(',')}]`),
}))

// Mock rerank
vi.mock('@/lib/search/rerank', () => ({
  rerank: vi.fn(),
  buildRerankText: vi.fn((c, p, h) => `${h}\n${p}\n\n${c}`),
}))
```

**Existing rerank tests (DO NOT duplicate):** 26 tests already exist at `tests/unit/search/rerank.test.ts` covering all rerank module behavior (passthrough, API calls, error handling, reordering). This story tests only the **wiring** — that `retrieveContext()` calls rerank correctly.

## Change Log

| Date       | Version | Description                     | Author     |
| ---------- | ------- | ------------------------------- | ---------- |
| 2026-02-18 | 0.1     | Initial draft — story creation  | Dev Agent  |
| 2026-03-01 | 1.0     | Full redraft: integrated Cohere Rerank v4 (from 14.3), deferred BM25 (per chunking-strategy.md decision), added architecture source references, comprehensive dev notes with existing module APIs, performance budget, testing strategy | Bob (SM) |
| 2026-03-01 | 1.1     | PO validation fixes: added AC 8-9 mapping to Task 1 with explicit subtasks, added query expansion deferral note to Context section, added `minRelevanceScore` option for future confidence thresholds, added structured logging format for timing instrumentation | Sarah (PO) |

## Dev Agent Record

### Agent Model Used

### Debug Log References

### Completion Notes List

### File List

## QA Results
