# Story 3.1: Set Up Vector Database for AI Chat

## Status
Draft

## Story

**As a** developer,
**I want** to ensure vector database is configured for AI chat functionality,
**so that** the chat interface can perform semantic similarity search for RAG.

## Acceptance Criteria

1. pgvector extension enabled in Supabase PostgreSQL (should already exist from Story 1.2)
2. Vector storage architecture finalized (embeddings in `legal_documents` table vs. separate `law_embeddings` table)
3. Test embeddings exist for chat testing (minimum 100 documents embedded)
4. Vector index created and optimized for chat query patterns
5. Chat-specific RAG retrieval function created
6. Function tested: "What are employee rights during sick leave?" returns relevant chunks
7. Query latency <500ms for similarity search
8. Documentation of vector search configuration for chat use case

## Tasks / Subtasks

- [ ] Verify pgvector extension (AC: 1)
  - [ ] Check Supabase for pgvector
  - [ ] If missing, enable extension
  - [ ] Verify vector type available

- [ ] Finalize storage architecture (AC: 2)
  - [ ] Decision: Monolithic vs. separate table
  - [ ] If separate: Create law_embeddings schema
  - [ ] If monolithic: Use legal_documents.embedding
  - [ ] Document decision and rationale

- [ ] Create test embeddings (AC: 3)
  - [ ] Select 100 representative documents
  - [ ] Generate embeddings using chosen strategy from 2.10b/c
  - [ ] Store in database
  - [ ] Verify storage successful

- [ ] Create/optimize vector index (AC: 4, 7)
  - [ ] Create HNSW index
  - [ ] Configure parameters for chat latency targets
  - [ ] Test query performance
  - [ ] Adjust parameters if needed

- [ ] Create RAG retrieval function (AC: 5, 6)
  - [ ] Function: retrieveRelevantChunks(query, k)
  - [ ] Generate query embedding
  - [ ] Perform vector similarity search
  - [ ] Return top-k chunks with metadata
  - [ ] Test with sample queries

- [ ] Document configuration (AC: 8)
  - [ ] Write vector search setup guide
  - [ ] Document index parameters
  - [ ] Note performance characteristics

## Dev Notes

### Storage Architecture Decision

**Option A: Monolithic (embeddings in legal_documents table)**
- Pros: Simple, single source of truth
- Cons: Larger table, potential performance impact

**Option B: Separate law_embeddings table**
- Pros: Better performance, cleaner separation
- Cons: Additional complexity, need to join tables

**Recommendation:** Separate `law_embeddings` table (likely chosen in 2.10b/c)

### Schema

**law_embeddings table:**
```sql
CREATE TABLE law_embeddings (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  document_id UUID NOT NULL REFERENCES legal_documents(id) ON DELETE CASCADE,
  chunk_index INTEGER NOT NULL,
  chunk_text TEXT NOT NULL,
  embedding vector(1536) NOT NULL,
  metadata JSONB,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),

  UNIQUE(document_id, chunk_index)
);

-- Vector index for similarity search
CREATE INDEX idx_law_embeddings_vector
ON law_embeddings
USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);
```

### RAG Retrieval Function

**lib/rag/retrieval.ts:**
```typescript
import { openai } from '@/lib/openai'
import { prisma } from '@/lib/prisma'

export interface RetrievalResult {
  chunkText: string
  documentId: string
  documentTitle: string
  documentNumber: string
  similarity: number
  metadata?: any
}

export async function retrieveRelevantChunks(
  query: string,
  k: number = 5
): Promise<RetrievalResult[]> {
  // Generate embedding for query
  const queryEmbedding = await openai.embeddings.create({
    model: 'text-embedding-3-small',
    input: query,
  })

  const embedding = queryEmbedding.data[0].embedding

  // Vector similarity search
  const results = await prisma.$queryRaw<RetrievalResult[]>`
    SELECT
      le.chunk_text AS "chunkText",
      le.document_id AS "documentId",
      ld.title AS "documentTitle",
      ld.document_number AS "documentNumber",
      1 - (le.embedding <=> ${embedding}::vector) AS similarity,
      le.metadata
    FROM law_embeddings le
    JOIN legal_documents ld ON le.document_id = ld.id
    ORDER BY le.embedding <=> ${embedding}::vector
    LIMIT ${k}
  `

  return results
}
```

### Test Query Example

```typescript
// Test retrieval
const results = await retrieveRelevantChunks(
  "What are employee rights during sick leave?",
  k = 5
)

console.log('Retrieved chunks:')
results.forEach((r, i) => {
  console.log(`\n[${i+1}] ${r.documentTitle} (${r.documentNumber})`)
  console.log(`Similarity: ${r.similarity.toFixed(3)}`)
  console.log(`Text: ${r.chunkText.substring(0, 200)}...`)
})
```

**Expected Output:**
```
Retrieved chunks:

[1] Lagen om sjuklön (SFS 1991:1047)
Similarity: 0.892
Text: § 7 Arbetstagare har rätt till sjuklön från arbetsgivaren för tid då arbetstagaren...

[2] Arbetsmiljölagen (SFS 1977:1160)
Similarity: 0.856
Text: § 3 Arbetsgivaren ska vidta alla åtgärder som behövs för att förebygga...

[3] Socialförsäkringsbalken (SFS 2010:110)
Similarity: 0.834
Text: Kap 27. § 2 Sjukpenning lämnas till den som på grund av sjukdom...
```

### Performance Optimization

**HNSW Index Parameters:**
- **m = 16:** Good balance of recall and build time
- **ef_construction = 64:** Quality build
- **ef_search = 40:** Runtime search parameter (can adjust)

**Query Optimization:**
```sql
-- Set search parameters for specific queries
SET hnsw.ef_search = 40; -- Higher = better recall, slower

-- Explain query plan
EXPLAIN ANALYZE
SELECT ...
FROM law_embeddings
ORDER BY embedding <=> query_vector
LIMIT 5;
```

### Important Architecture References

**From Stories 2.10a-c:**
- Chunking strategy chosen
- Embedding generation approach

**From Architecture Section 11.5:**
- RAG pipeline design
- Vector search configuration

**From Architecture Section 9:**
- Database schema for vector storage

### Testing

**Test File:** `tests/integration/rag/vector-search.test.ts`

```typescript
describe('Vector Search', () => {
  it('retrieves relevant chunks for query', async () => {
    const results = await retrieveRelevantChunks(
      "What are employee rights during sick leave?",
      5
    )

    expect(results).toHaveLength(5)
    expect(results[0].similarity).toBeGreaterThan(0.7)
    expect(results[0].chunkText).toBeDefined()
  })

  it('meets latency target <500ms', async () => {
    const start = Date.now()

    await retrieveRelevantChunks("test query", 5)

    const latency = Date.now() - start
    expect(latency).toBeLessThan(500)
  })
})
```

### User Responsibilities
- None for this story

### Developer Responsibilities
- Finalize vector storage architecture
- Create test embeddings
- Configure vector index
- Implement retrieval function
- Test and optimize performance
- Document configuration

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|---------|
| 2025-11-11 | 1.0 | Initial story creation | Sarah (PO) |

## Dev Agent Record
_To be populated by dev agent_

## QA Results
_To be populated by QA agent_
