# Story 3.2: Implement RAG Query Pipeline

## Status

Archived — Superseded by Epic 14 (Stories 14.7 + 14.8 + 14.9). The RAG pipeline was redesigned as an agentic architecture: retrieval via `retrieveContext()` (14.8), agent tools call retrieval and present results (14.7), system prompt with behavioral guardrails (14.9). Chat route already exists at `app/api/chat/route.ts` (Story 3.3/14.11).

## Story

**As a** developer,
**I want** to build a RAG pipeline that retrieves relevant law chunks and generates answers,
**so that** the AI chatbot can respond accurately to user questions.

## Acceptance Criteria

1. API endpoint created: `POST /api/chat/query`
2. Request body: `{ query: string, context?: string[] }`
3. Pipeline steps: Generate query embedding → Vector search top 10 chunks → Construct prompt → Call GPT-4 → Return with citations
4. System prompt enforces: "ONLY answer from provided chunks, cite sources"
5. Response format: `{ answer: string, citations: [{ law_id, sfs_number, title, chunk_text }] }`
6. RAG accuracy tested with 20 sample questions (>90% grounded answers)
7. Query latency <3 seconds end-to-end
8. Error handling: LLM timeout, no relevant chunks
9. Logging: User query, retrieved chunks, LLM response

## Tasks / Subtasks

- [ ] Create API endpoint (AC: 1, 2)
- [ ] Implement RAG pipeline (AC: 3)
- [ ] Create system prompt (AC: 4)
- [ ] Format response with citations (AC: 5)
- [ ] Test with sample questions (AC: 6)
- [ ] Optimize latency (AC: 7)
- [ ] Add error handling (AC: 8)
- [ ] Implement logging (AC: 9)

## Dev Notes

**API Route:** `app/api/chat/query/route.ts`

**RAG Pipeline:**

```typescript
export async function POST(request: Request) {
  const { query, context } = await request.json()

  // 1. Retrieve relevant chunks
  const chunks = await retrieveRelevantChunks(query, 10)

  // 2. Construct prompt
  const prompt = buildRAGPrompt(query, chunks)

  // 3. Call GPT-4
  const answer = await generateAnswer(prompt)

  // 4. Extract citations
  const citations = extractCitations(answer, chunks)

  return NextResponse.json({ answer, citations })
}
```

**System Prompt:**

```
You are a Swedish legal assistant. Answer ONLY based on the provided legal text chunks. Always cite your sources using [1], [2] format. If you cannot answer from the provided chunks, say "Jag har inte tillräcklig information för att svara på den frågan."
```

**Reference:** Architecture Section 11.5 (RAG Pipeline)

## Change Log

| Date       | Version | Description            | Author     |
| ---------- | ------- | ---------------------- | ---------- |
| 2025-11-11 | 1.0     | Initial story creation | Sarah (PO) |

## Dev Agent Record

_To be populated by dev agent_

## QA Results

_To be populated by QA agent_
