# Story 2.10c: Implement Chosen RAG Chunking Strategy

## Status

Archived — Superseded by Epic 14 (Stories 14.2 + 14.3). All deliverables completed: ContentChunk model + three-tier chunking pipeline (14.2), 228,778 chunks embedded with HNSW index + Cohere Rerank v4 (14.3). Storage uses unified `ContentChunk` table with pgvector, not the separate `law_embeddings` table proposed here.

## Story

**As a** developer,
**I want** to implement the production RAG chunking strategy based on experimental results,
**so that** all 170,000+ documents are properly chunked and embedded for semantic search.

## Acceptance Criteria

1. Production chunking implementation based on Story 2.10b experimental winner
2. Chunking logic implemented per content type (may differ for SFS/Court/EU)
3. Production embedding generation script created
4. All 170,000+ documents chunked according to chosen strategy
5. Embeddings generated using OpenAI `text-embedding-3-small` (1536 dimensions)
6. Chunk metadata stored: document_id, chunk_index, content_type, structural_info
7. Embeddings stored in `legal_documents.embedding` vector field or separate `law_embeddings` table (based on architecture decision)
8. HNSW vector index created for fast similarity search
9. Rate limiting implemented (max 1,000 req/min for OpenAI)
10. Progress logging per content type
11. Script completes in <16 hours for all documents
12. Test query validates retrieval quality matches experimental results
13. Verification: Database contains expected number of chunk embeddings
14. Cost tracking: Total embedding generation cost logged

## Tasks / Subtasks

- [ ] Implement production chunking functions (AC: 1, 2)
  - [ ] Create `lib/rag/chunking.ts` based on chosen strategy
  - [ ] Implement per-content-type logic (if strategies differ)
  - [ ] Handle edge cases (very short/long documents)
  - [ ] Add unit tests for chunking logic

- [ ] Create production embedding script (AC: 3, 9, 10)
  - [ ] Create `scripts/generate-embeddings.ts`
  - [ ] Batch processing with rate limiting
  - [ ] Progress tracking and resumption support
  - [ ] Error handling and retry logic

- [ ] Chunk and embed all documents (AC: 4, 5)
  - [ ] Run script on all 170,000+ documents
  - [ ] Monitor progress and costs
  - [ ] Handle failures gracefully

- [ ] Store chunks and embeddings (AC: 6, 7)
  - [ ] Decide storage architecture (monolithic vs. separate table)
  - [ ] Insert chunks with metadata
  - [ ] Store embeddings in vector fields

- [ ] Create vector index (AC: 8)
  - [ ] Create HNSW index on embedding column
  - [ ] Configure index parameters (m, ef_construction)
  - [ ] Verify index performance

- [ ] Test and validate (AC: 12, 13, 14)
  - [ ] Run test queries from Story 2.10a
  - [ ] Verify retrieval quality matches experiments
  - [ ] Check total chunk count
  - [ ] Calculate and log total cost

## Dev Notes

### Implementation Based on Experimental Results

**This story implements whatever strategy won in Story 2.10b.**

For example, if experiments showed:

- **SFS Laws:** Structural chunking by § (max 800 tokens, 50 token overlap)
- **Court Cases:** Semantic chunking by paragraph (max 1000 tokens, 25 token overlap)
- **EU Documents:** Structural chunking by article (max 600 tokens, 50 token overlap)

Then we implement those specific strategies.

### Production Chunking Implementation Example

**lib/rag/chunking.ts:**

```typescript
import { LegalDocument } from '@prisma/client'

interface ChunkingStrategy {
  contentType: string
  method: 'structural' | 'semantic' | 'fixed'
  delimiter?: string
  maxTokens: number
  overlap: number
}

// Load chosen strategies from experimental results
const STRATEGIES: ChunkingStrategy[] = [
  {
    contentType: 'SFS_LAW',
    method: 'structural',
    delimiter: '§',
    maxTokens: 800,
    overlap: 50,
  },
  {
    contentType: 'HD_SUPREME_COURT',
    method: 'semantic',
    delimiter: 'paragraph',
    maxTokens: 1000,
    overlap: 25,
  },
  // ... etc based on experiments
]

export async function chunkDocument(doc: LegalDocument): Promise<Chunk[]> {
  const strategy = STRATEGIES.find((s) => s.contentType === doc.contentType)

  if (!strategy) {
    throw new Error(`No chunking strategy for content type: ${doc.contentType}`)
  }

  switch (strategy.method) {
    case 'structural':
      return chunkByStructure(doc, strategy)
    case 'semantic':
      return chunkBySemantic(doc, strategy)
    case 'fixed':
      return chunkByTokens(doc, strategy)
  }
}

function chunkByStructure(
  doc: LegalDocument,
  strategy: ChunkingStrategy
): Chunk[] {
  const chunks: Chunk[] = []

  // Example for SFS laws (chunk by §)
  if (strategy.delimiter === '§') {
    const sections = doc.fullText.split(/\n\s*§\s*\d+/)

    for (let i = 0; i < sections.length; i++) {
      const sectionText = sections[i]
      const tokens = countTokens(sectionText)

      // If section exceeds maxTokens, split it further
      if (tokens > strategy.maxTokens) {
        const subChunks = splitLargeChunk(
          sectionText,
          strategy.maxTokens,
          strategy.overlap
        )
        chunks.push(
          ...subChunks.map((text, j) => ({
            documentId: doc.id,
            chunkIndex: i * 100 + j, // Preserve ordering
            text,
            metadata: {
              section: extractSectionNumber(sectionText),
              contentType: doc.contentType,
              isSplit: true,
            },
          }))
        )
      } else {
        chunks.push({
          documentId: doc.id,
          chunkIndex: i,
          text: sectionText,
          metadata: {
            section: extractSectionNumber(sectionText),
            contentType: doc.contentType,
          },
        })
      }
    }
  }

  // Add overlap between chunks
  return addOverlap(chunks, strategy.overlap)
}

function addOverlap(chunks: Chunk[], overlapTokens: number): Chunk[] {
  if (overlapTokens === 0) return chunks

  for (let i = 1; i < chunks.length; i++) {
    const previousChunk = chunks[i - 1]
    const currentChunk = chunks[i]

    // Take last N tokens from previous chunk
    const overlapText = getLastNTokens(previousChunk.text, overlapTokens)

    // Prepend to current chunk
    currentChunk.text = overlapText + ' ' + currentChunk.text
  }

  return chunks
}
```

### Production Embedding Script

**scripts/generate-embeddings.ts:**

```typescript
import { prisma } from '@/lib/prisma'
import { openai } from '@/lib/openai'
import { chunkDocument } from '@/lib/rag/chunking'
import pLimit from 'p-limit'

const limit = pLimit(50) // 50 concurrent requests (within 1000/min limit)

async function generateAllEmbeddings() {
  const documents = await prisma.legalDocument.findMany({
    where: {
      // Only documents without embeddings
      embedding: null,
    },
  })

  console.log(`Generating embeddings for ${documents.length} documents`)

  let totalChunks = 0
  let totalCost = 0

  for (const doc of documents) {
    try {
      // Chunk document
      const chunks = await chunkDocument(doc)
      totalChunks += chunks.length

      // Generate embeddings in parallel (with rate limiting)
      const embeddingPromises = chunks.map((chunk) =>
        limit(async () => {
          const response = await openai.embeddings.create({
            model: 'text-embedding-3-small',
            input: chunk.text,
          })

          return {
            ...chunk,
            embedding: response.data[0].embedding,
          }
        })
      )

      const chunksWithEmbeddings = await Promise.all(embeddingPromises)

      // Store in database
      await storeChunks(doc.id, chunksWithEmbeddings)

      // Calculate cost
      const cost = calculateCost(chunks, 'text-embedding-3-small')
      totalCost += cost

      console.log(
        `✓ ${doc.documentNumber}: ${chunks.length} chunks, €${cost.toFixed(4)}`
      )
    } catch (error) {
      console.error(`✗ Failed to process ${doc.documentNumber}:`, error)
      // Log to Sentry but continue
    }
  }

  console.log(
    `\n✅ Complete: ${totalChunks} chunks, €${totalCost.toFixed(2)} total cost`
  )
}

async function storeChunks(documentId: string, chunks: ChunkWithEmbedding[]) {
  // Decision: Use separate law_embeddings table for better performance
  await prisma.lawEmbedding.createMany({
    data: chunks.map((chunk) => ({
      documentId,
      chunkIndex: chunk.chunkIndex,
      chunkText: chunk.text,
      embedding: chunk.embedding,
      metadata: chunk.metadata,
    })),
  })
}

function calculateCost(chunks: Chunk[], model: string): number {
  const totalTokens = chunks.reduce((sum, c) => sum + countTokens(c.text), 0)

  // text-embedding-3-small: $0.02 per 1M tokens
  return (totalTokens / 1_000_000) * 0.02
}
```

### Vector Index Creation

**After all embeddings generated:**

```sql
-- Create HNSW index for fast similarity search
CREATE INDEX idx_law_embeddings_vector
ON law_embeddings
USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);

-- Analyze table for query optimization
ANALYZE law_embeddings;
```

**Index Configuration:**

- **m = 16:** Number of connections per layer (higher = better recall, slower build)
- **ef_construction = 64:** Size of dynamic candidate list (higher = better index quality)
- **vector_cosine_ops:** Cosine similarity distance (best for normalized embeddings)

### Testing RAG Retrieval

**Test query from Story 2.10a:**

```typescript
async function testRetrieval() {
  const query = 'What are employee rights during sick leave?'

  // Generate query embedding
  const queryEmbedding = await openai.embeddings.create({
    model: 'text-embedding-3-small',
    input: query,
  })

  // Vector similarity search
  const results = await prisma.$queryRaw`
    SELECT
      law_embeddings.chunk_text,
      legal_documents.title,
      legal_documents.document_number,
      1 - (law_embeddings.embedding <=> ${queryEmbedding.data[0].embedding}::vector) as similarity
    FROM law_embeddings
    JOIN legal_documents ON law_embeddings.document_id = legal_documents.id
    ORDER BY law_embeddings.embedding <=> ${queryEmbedding.data[0].embedding}::vector
    LIMIT 10
  `

  console.log('Top 10 retrieved chunks:')
  results.forEach((r, i) => {
    console.log(`\n[${i + 1}] ${r.title} (${r.document_number})`)
    console.log(`Similarity: ${r.similarity.toFixed(3)}`)
    console.log(`Chunk: ${r.chunk_text.substring(0, 200)}...`)
  })
}
```

**Verification:** Retrieved chunks should match quality from Story 2.10b experiments.

### Cost Estimation

**Embedding Generation Cost:**

- 170,000 documents
- Average 10 chunks per document = 1,700,000 chunks
- Average 400 tokens per chunk = 680,000,000 tokens
- Cost: 680M tokens × $0.02 / 1M = **~$13.60 total**

**Storage Cost:**

- 1,700,000 chunks × 1536 dimensions × 4 bytes = ~10.4GB
- Supabase free tier: 500MB, Pro tier: 8GB (need upgrade)
- Estimated: **$25/month** for database storage

### Important Architecture References

**From Story 2.10b:**

- Experimental results and chosen strategies
- Performance baselines

**From Architecture Section 9:**

- Database schema for law_embeddings table
- Vector index specifications

**From Architecture Section 11.5:**

- RAG pipeline implementation details

### Testing

**Test File:** `tests/integration/rag/chunking.test.ts`

```typescript
describe('Production Chunking', () => {
  it('chunks SFS law according to chosen strategy', async () => {
    const law = await createTestLaw()
    const chunks = await chunkDocument(law)

    // Verify strategy applied correctly
    expect(chunks.length).toBeGreaterThan(0)
    expect(chunks.every((c) => countTokens(c.text) <= 800)).toBe(true)
    expect(chunks[0].metadata.contentType).toBe('SFS_LAW')
  })

  it('retrieves relevant chunks for test query', async () => {
    const query = 'What are employee rights during sick leave?'
    const results = await vectorSearch(query, (k = 10))

    expect(results.length).toBe(10)
    expect(results[0].similarity).toBeGreaterThan(0.7)
  })
})
```

### User Responsibilities

- None for this story

### Developer Responsibilities

- Implement chosen chunking strategy accurately
- Run embedding generation script
- Monitor costs and progress
- Create vector index
- Validate retrieval quality
- Document final implementation

## Change Log

| Date       | Version | Description                              | Author     |
| ---------- | ------- | ---------------------------------------- | ---------- |
| 2025-11-11 | 1.0     | Initial story creation (split from 2.10) | Sarah (PO) |

## Dev Agent Record

_To be populated by dev agent_

## QA Results

_To be populated by QA agent_
