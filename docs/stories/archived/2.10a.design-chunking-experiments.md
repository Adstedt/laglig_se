# Story 2.10a: Design RAG Chunking Experiments and Testing Framework

## Status

Archived — Superseded by Epic 14 (Stories 14.2 + 14.3). The experimental approach to chunking strategy was replaced by an informed architectural decision: paragraf (§) as the semantic atom of Swedish law, documented in `docs/architecture/chunking-strategy.md`. Retrieval quality validated via Cohere Rerank v4 benchmarking in Story 14.3.

## Story

**As a** developer,
**I want** to design a systematic testing framework for evaluating RAG chunking strategies,
**so that** I can make data-driven decisions about optimal chunking approaches for each content type.

## Acceptance Criteria

1. Testing framework created to evaluate chunking strategies
2. Test dataset defined: 100 representative documents (SFS laws, court cases, EU docs)
3. 50 test queries created covering common user questions
4. Evaluation metrics defined:
   - Retrieval accuracy (did we get the right chunks?)
   - Semantic coherence (do chunks make sense?)
   - Context preservation (is necessary context included?)
   - Query response quality (does RAG produce good answers?)
5. Multiple chunking strategies identified for testing:
   - Fixed token chunking (300, 500, 800, 1000 tokens)
   - Semantic chunking (sentence boundaries, paragraph boundaries)
   - Structural chunking (by § for laws, by article for EU, by section for cases)
   - Hybrid approaches (structural + semantic + size limits)
6. Overlap strategies to test: 0, 25, 50, 100 tokens
7. Test harness script created to run experiments
8. Results collection methodology defined (CSV/JSON output with metrics)
9. Comparison visualization planned (charts showing strategy performance)
10. Documentation of experimental approach

## Tasks / Subtasks

- [ ] Create test dataset (AC: 2)
  - [ ] Select 40 representative SFS laws (varied length, structure)
  - [ ] Select 30 representative court cases (HD, HovR, HFD)
  - [ ] Select 30 representative EU documents (regulations, directives)
  - [ ] Ensure diversity in length, complexity, subject matter

- [ ] Create test query set (AC: 3)
  - [ ] 50 realistic user questions
  - [ ] Cover different query types: factual, procedural, comparative
  - [ ] Include queries requiring different context amounts
  - [ ] Document expected relevant chunks for each query

- [ ] Define evaluation metrics (AC: 4)
  - [ ] Retrieval accuracy: Precision@K, Recall@K (K=5, 10, 20)
  - [ ] Semantic coherence: Manual scoring 1-5 scale
  - [ ] Context preservation: Check if citations/references preserved
  - [ ] RAG quality: GPT-4 evaluation of generated answers
  - [ ] Response time: Embedding + retrieval latency

- [ ] Identify chunking strategies to test (AC: 5, 6)
  - [ ] Fixed token: 300, 500, 800, 1000 tokens
  - [ ] Semantic: By sentence, by paragraph
  - [ ] Structural: By § (laws), by article (EU), by heading (cases)
  - [ ] Hybrid: Structural with size limits (e.g., § up to 800 tokens)
  - [ ] Overlap: 0, 25, 50, 100 tokens between chunks

- [ ] Create test harness (AC: 7, 8)
  - [ ] Script to chunk documents with each strategy
  - [ ] Generate embeddings for all chunk variants
  - [ ] Run test queries against each chunking approach
  - [ ] Collect metrics for each run
  - [ ] Output results to structured format (JSON/CSV)

- [ ] Plan comparison visualization (AC: 9)
  - [ ] Charts: Retrieval accuracy by strategy
  - [ ] Tables: Performance comparison matrix
  - [ ] Heat maps: Strategy performance by content type

- [ ] Document experimental design (AC: 10)
  - [ ] Write methodology document
  - [ ] Hypothesis for each strategy
  - [ ] Expected trade-offs
  - [ ] Decision criteria for selecting winner

## Dev Notes

### Experimental Design Philosophy

**Key Principle:** We DON'T know the optimal chunking strategy yet. Different content types may need different approaches. We must test and measure.

**Hypotheses to Test:**

1. **H1:** Structural chunking (by §) preserves legal context better than fixed tokens
2. **H2:** Larger chunks (800-1000 tokens) improve context but reduce precision
3. **H3:** Overlap improves retrieval recall but increases storage/cost
4. **H4:** Court cases need different chunking than laws (narrative vs. structured)
5. **H5:** EU documents with preambles need special handling

### Test Dataset Design

**Selection Criteria:**

- **Variety in structure:** Simple laws (few §), complex laws (many chapters)
- **Variety in length:** Short (500 words) to long (50,000 words)
- **Content diversity:** Different legal domains (employment, tax, environment)
- **Real-world importance:** Laws users actually ask about

**Example Test Documents:**

- Arbetsmiljölagen (structured, medium length, high query volume)
- GDPR/Dataskyddsförordningen (EU, complex, long)
- Simple regulation (short, straightforward)
- HD court case (narrative structure)

### Test Query Design

**Query Categories:**

1. **Factual queries:** "What is the minimum wage in Sweden?"
2. **Procedural queries:** "How do I register a company?"
3. **Comparative queries:** "What's the difference between AB and HB?"
4. **Contextual queries:** "What are employer obligations during sick leave?"
5. **Specific section queries:** "What does § 7 of Arbetsmiljölagen say?"

**Expected Behavior:**

- Factual: Should retrieve 1-2 highly relevant chunks
- Procedural: May need 5-10 chunks across multiple laws
- Comparative: Needs chunks from multiple documents
- Contextual: Needs surrounding context preserved in chunks

### Evaluation Metrics Details

**1. Retrieval Accuracy (Automated):**

```typescript
// For each test query, manually tag relevant chunks
const relevantChunks = ['chunk_123', 'chunk_456', 'chunk_789']

// Measure Precision@K and Recall@K
function evaluateRetrieval(
  retrievedChunks: string[],
  relevantChunks: string[],
  k: number
) {
  const topK = retrievedChunks.slice(0, k)
  const hits = topK.filter((c) => relevantChunks.includes(c)).length

  const precision = hits / k
  const recall = hits / relevantChunks.length

  return {
    precision,
    recall,
    f1: (2 * (precision * recall)) / (precision + recall),
  }
}
```

**2. Semantic Coherence (Manual):**

- Score 1-5: Does chunk make sense standalone?
- Can you understand the chunk without reading the full document?
- Are sentences cut off mid-thought?

**3. Context Preservation (Automated Check):**

- Does chunk include section headers?
- Are references to "previous section" included?
- Is chapter context preserved?

**4. RAG Answer Quality (GPT-4 Evaluation):**

```typescript
// Use GPT-4 to evaluate answer quality
const evaluationPrompt = `
Rate this RAG answer on 1-10 scale:
- Accuracy (is it correct?)
- Completeness (does it fully answer?)
- Citations (are sources cited properly?)

Question: ${query}
Retrieved chunks: ${chunks}
Generated answer: ${answer}
`
```

### Test Harness Structure

**scripts/test-chunking-strategies.ts:**

```typescript
const strategies = [
  { name: 'fixed-300', type: 'fixed', tokens: 300, overlap: 0 },
  { name: 'fixed-500', type: 'fixed', tokens: 500, overlap: 50 },
  {
    name: 'structural-section',
    type: 'structural',
    delimiter: '§',
    maxTokens: 800,
  },
  { name: 'semantic-paragraph', type: 'semantic', delimiter: 'paragraph' },
  // ... more strategies
]

async function runExperiment() {
  const results = []

  for (const strategy of strategies) {
    console.log(`Testing: ${strategy.name}`)

    // Chunk all test documents
    const chunks = await chunkDocuments(testDataset, strategy)

    // Generate embeddings
    const embeddings = await generateEmbeddings(chunks)

    // Run all test queries
    for (const query of testQueries) {
      const retrieved = await searchSimilar(query, embeddings, (k = 10))
      const metrics = evaluateRetrieval(
        retrieved,
        query.relevantChunks,
        (k = 10)
      )

      results.push({
        strategy: strategy.name,
        query: query.id,
        ...metrics,
      })
    }
  }

  // Save results
  await saveResults(results, 'chunking-experiment-results.json')
}
```

### Decision Criteria

**How we'll choose the winning strategy:**

1. **Retrieval Accuracy:** F1 score must be >0.7
2. **Semantic Coherence:** Average score >4.0/5.0
3. **RAG Answer Quality:** Average score >7.0/10.0
4. **Performance:** <500ms retrieval time
5. **Cost:** Embedding generation cost per document <€0.01

**Trade-offs to consider:**

- Smaller chunks = better precision, worse context
- Larger chunks = better context, worse precision
- More overlap = better recall, higher storage cost
- Structural chunking = preserves meaning, variable chunk size
- Fixed chunking = predictable cost, may break semantics

### Important Architecture References

**From Architecture Section 3 (RAG Strategy):**

- Initial RAG design thoughts
- Embedding model selection (OpenAI text-embedding-3-small)

**From docs/external-apis/README.md:**

- OpenAI API costs and rate limits

### Testing

**This story IS the testing framework creation.**

After this story, Story 2.10b will execute the experiments, and Story 2.10c will implement the chosen strategy.

### User Responsibilities

- None for this story (developer research task)

### Developer Responsibilities

- Design rigorous testing methodology
- Create comprehensive test dataset
- Build automated test harness
- Document experimental approach
- Prepare for data-driven decision making in Story 2.10b

## Change Log

| Date       | Version | Description                                    | Author     |
| ---------- | ------- | ---------------------------------------------- | ---------- |
| 2025-11-11 | 1.0     | Initial story creation                         | Sarah (PO) |
| 2025-11-11 | 1.1     | Split from 2.10 to reflect experimental nature | Sarah (PO) |

## Dev Agent Record

_To be populated by dev agent_

## QA Results

_To be populated by QA agent_
