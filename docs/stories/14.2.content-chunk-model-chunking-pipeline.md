# Story 14.2: ContentChunk Model & Chunking Pipeline

## Status

Draft

## Story

**As a** platform building semantic search,
**I want** legal documents broken into paragraph-level chunks with contextual headers stored in a unified table,
**so that** the agent can retrieve precisely relevant content with full document context.

## Context & Dependencies

**Why this story exists:**

The existing `LegalDocument.embedding` field holds a single 1536-dim vector for the entire document. This is too coarse for RAG — a search for "employer obligations for personal protective equipment" should return the specific paragraph (e.g., Arbetsmiljolagen 2 kap. 7 §), not the entire 50-page law. This story creates the `ContentChunk` model and a chunking pipeline that breaks documents into paragraph-level chunks with contextual headers (breadcrumbs), enabling precise retrieval in the RAG pipeline.

**Builds on:**

- Story 14.1 (Standardized Document JSON Schema) — provides `json_content` in canonical hierarchical schema with `chapters > sections > paragraphs`
- `LegalDocument` model — `json_content` (Json?), `content_type` (ContentType enum) [Source: prisma/schema.prisma:281-321]
- pgvector extension — already enabled in Supabase, used for `LegalDocument.embedding` with `Unsupported("vector(1536)")` type [Source: prisma/schema.prisma:301]
- Prisma schema conventions — `@@map()` for table names, `@default(cuid())` for IDs, `@db.Text` for long strings [Source: prisma/schema.prisma]

**Depends on:** Story 14.1 must be complete (provides canonical `json_content` that the chunking function consumes).

**Depended on by:**

- Story 14.3 (Embedding Generation Pipeline) — generates embeddings for chunks created by this story
- All RAG retrieval stories — query ContentChunk with vector similarity search
- Future stories for USER_FILE, CONVERSATION, and ASSESSMENT chunking (14.5, 14.6) — reuse the same ContentChunk model

## Acceptance Criteria

### Prisma Model

1. Prisma model `ContentChunk` created in `prisma/schema.prisma` with fields: `id` (cuid), `sourceType` (SourceType enum), `sourceId` (String), `workspaceId` (String?, nullable), `path` (String), `contextualHeader` (String), `content` (String @db.Text), `contentRole` (ContentRole enum), `embedding` (Unsupported("vector(1536)")?), `tokenCount` (Int), `metadata` (Json?), `createdAt` (DateTime), `updatedAt` (DateTime)
2. pgvector extension enabled and embedding field uses `Unsupported("vector(1536)")` type (matching existing pattern on `LegalDocument`)
3. `SourceType` enum created: `LEGAL_DOCUMENT`, `USER_FILE`, `CONVERSATION`, `ASSESSMENT`
4. `workspaceId` is nullable — `NULL` for shared legal documents (SFS laws, amendments, agency regulations), set for workspace-private content (user files, conversations)
5. `path` field encodes hierarchical position using convention `kap{N}.§{N}.st{N}` (e.g., `kap2.§3.st1`)
6. `contextualHeader` contains full breadcrumb string (e.g., "Arbetsmiljolagen (SFS 1977:1160) > Kap 2: Arbetsmiljons beskaffenhet > 3 &sect;")
7. `ContentRole` enum created (matching Story 14.1): `PARAGRAPH`, `ALLMANT_RAD`, `TABLE`, `HEADING`, `TRANSITION_PROVISION`, `FOOTNOTE`

### Indexes

11. pgvector HNSW index created on `embedding` column with cosine distance operator
12. Database migration runs cleanly with `prisma migrate dev`
13. Composite index on `(sourceType, sourceId)` for efficient "find all chunks for a document" lookups
14. Index on `workspaceId` for workspace-scoped queries

### Chunking Pipeline

8. Chunking function `chunkDocument` in `lib/chunks/chunk-document.ts` derives chunks from `json_content` (Story 14.1 canonical schema)
9. Contextual header generated from: document title + chapter title + section number (e.g., "Arbetsmiljolagen (SFS 1977:1160) > Kap 2: Arbetsmiljons beskaffenhet > 3 &sect;")
10. Chunk lifecycle: when a document's `json_content` changes, old chunks are deleted and new ones generated (atomic via transaction)

### Testing

15. At least 10 unit tests for chunking logic

## Tasks / Subtasks

- [ ] **Task 1: Prisma schema — ContentChunk model** (AC: 1-7, 12)
  - [ ] Add `SourceType` enum to `prisma/schema.prisma`: `LEGAL_DOCUMENT`, `USER_FILE`, `CONVERSATION`, `ASSESSMENT`
  - [ ] Add `ContentRole` enum to `prisma/schema.prisma`: `PARAGRAPH`, `ALLMANT_RAD`, `TABLE`, `HEADING`, `TRANSITION_PROVISION`, `FOOTNOTE`
  - [ ] Add `ContentChunk` model with all fields:
    ```prisma
    model ContentChunk {
      id               String                       @id @default(cuid())
      source_type      SourceType
      source_id        String                       // FK-like reference to source (LegalDocument.id, etc.)
      workspace_id     String?                      // NULL for shared legal docs, set for workspace-private
      path             String                       // Hierarchical position: "kap2.§3.st1"
      contextual_header String                      // Full breadcrumb for retrieval context
      content          String                       @db.Text // The chunk text
      content_role     ContentRole                  // PARAGRAPH, TABLE, etc.
      embedding        Unsupported("vector(1536)")? // Filled by Story 14.3
      token_count      Int                          // Approximate token count for budgeting
      metadata         Json?                        // Flexible metadata (change_type, etc.)
      created_at       DateTime                     @default(now())
      updated_at       DateTime                     @updatedAt

      @@index([source_type, source_id])
      @@index([workspace_id])
      @@map("content_chunks")
    }
    ```
  - [ ] Run `pnpm prisma migrate dev --name add-content-chunk-model` to create migration
  - [ ] Run `pnpm prisma generate` to update Prisma client types
  - [ ] Verify migration succeeds on dev database

- [ ] **Task 2: pgvector HNSW index** (AC: 11)
  - [ ] Add raw SQL to the migration for HNSW index creation:
    ```sql
    CREATE INDEX content_chunks_embedding_idx ON content_chunks
      USING hnsw (embedding vector_cosine_ops)
      WITH (m = 16, ef_construction = 64);
    ```
  - [ ] Verify index is created by querying `pg_indexes` after migration
  - [ ] Note: Prisma cannot express HNSW indexes natively — raw SQL in migration is the established pattern (same as `LegalDocument.embedding`)

- [ ] **Task 3: Chunking function** (AC: 8-9)
  - [ ] Create `lib/chunks/chunk-document.ts`
  - [ ] Implement `chunkDocument(document: { id: string, title: string, documentNumber: string, contentType: ContentType, jsonContent: CanonicalDocumentJson }): ChunkCreateInput[]`
  - [ ] Iterate through canonical JSON: `chapters > sections > paragraphs`
  - [ ] For each paragraph, generate:
    - `sourceType`: `LEGAL_DOCUMENT`
    - `sourceId`: `document.id`
    - `workspaceId`: `null` (legal documents are shared)
    - `path`: `kap{chapter.number}.§{section.number}.st{paragraph.number}` — use `kap0` for implicit chapters
    - `contextualHeader`: `"{title} ({documentNumber}) > Kap {N}: {chapterTitle} > {section.number} §"` — omit chapter part for implicit chapters
    - `content`: paragraph text
    - `contentRole`: paragraph role from canonical schema
    - `tokenCount`: estimated token count
    - `metadata`: optional — store `changeType` for amendments, `source` for agency regulations
  - [ ] Handle edge cases:
    - Implicit chapter (number: null) → path starts with `kap0.§...`, header omits chapter
    - Section without paragraphs → skip (log warning)
    - Very long paragraphs (> 8000 tokens) → log warning but do not split (future optimization)
  - [ ] Export `ChunkCreateInput` type for downstream use

- [ ] **Task 4: Chunk lifecycle sync** (AC: 10)
  - [ ] Create `lib/chunks/sync-document-chunks.ts`
  - [ ] Implement `syncDocumentChunks(documentId: string): Promise<SyncResult>`
  - [ ] Load document from DB with `json_content`, `title`, `document_number`, `content_type`
  - [ ] Parse `json_content` as `CanonicalDocumentJson`
  - [ ] Delete all existing chunks for this document: `prisma.contentChunk.deleteMany({ where: { source_type: 'LEGAL_DOCUMENT', source_id: documentId } })`
  - [ ] Generate new chunks via `chunkDocument()`
  - [ ] Create all new chunks: `prisma.contentChunk.createMany({ data: chunks })`
  - [ ] Wrap delete + create in a Prisma transaction (`prisma.$transaction([...])`) for atomicity
  - [ ] Return `SyncResult`: `{ documentId, chunksDeleted, chunksCreated, duration }`
  - [ ] Handle error: if document has no `json_content`, log warning and skip

- [ ] **Task 5: Token counting** (AC: 1)
  - [ ] Create `lib/chunks/token-count.ts`
  - [ ] Implement `estimateTokenCount(text: string): number`
  - [ ] Use simple heuristic: `Math.ceil(text.length / 4)` as rough estimate (characters / 4 approximates GPT tokenization)
  - [ ] Optionally: if `js-tiktoken` is in package.json, use `encoding_for_model('text-embedding-3-small')` for precise count
  - [ ] Document the estimation method in JSDoc
  - [ ] Export for use in chunking function and embedding cost estimation (Story 14.3)

- [ ] **Task 6: Tests** (AC: 15)
  - [ ] Create `tests/unit/chunks/chunk-document.test.ts` — at least 7 tests
  - [ ] Create `tests/unit/chunks/sync-document-chunks.test.ts` — at least 3 tests
  - [ ] Verify `npx tsc --noEmit` passes
  - [ ] Verify all existing tests still pass

## Dev Notes

### Source Tree

```
prisma/
  └── schema.prisma                      — MODIFIED — add ContentChunk model, SourceType + ContentRole enums
  └── migrations/
      └── YYYYMMDD_add_content_chunk_model/
          └── migration.sql              — NEW — includes HNSW index creation

lib/chunks/                              — NEW directory
  ├── chunk-document.ts                  — chunkDocument() — derives chunks from json_content
  ├── sync-document-chunks.ts            — syncDocumentChunks() — delete old + create new in transaction
  ├── token-count.ts                     — estimateTokenCount() — token estimation
  └── index.ts                           — barrel re-exports

tests/unit/chunks/                       — NEW directory
  ├── chunk-document.test.ts             — chunking logic tests
  └── sync-document-chunks.test.ts       — lifecycle sync tests
```

### pgvector in Prisma

Prisma does not natively support pgvector types. The established pattern in this project (from `LegalDocument.embedding`) is:

```prisma
embedding Unsupported("vector(1536)")?
```

The HNSW index must be created via raw SQL in the migration file:

```sql
CREATE INDEX content_chunks_embedding_idx ON content_chunks
  USING hnsw (embedding vector_cosine_ops)
  WITH (m = 16, ef_construction = 64);
```

Parameters: `m = 16` (number of connections per layer), `ef_construction = 64` (build-time search quality). These are recommended for datasets under 1M vectors. [Source: pgvector documentation]

For reading/writing embeddings, use Prisma `$queryRaw` / `$executeRaw`:

```typescript
// Writing embedding
await prisma.$executeRaw`
  UPDATE content_chunks SET embedding = ${vector}::vector
  WHERE id = ${chunkId}
`

// Reading with similarity search
const results = await prisma.$queryRaw`
  SELECT id, content, contextual_header,
    1 - (embedding <=> ${queryVector}::vector) as similarity
  FROM content_chunks
  WHERE source_type = 'LEGAL_DOCUMENT'
  ORDER BY embedding <=> ${queryVector}::vector
  LIMIT 10
`
```

### Path Encoding Convention

The `path` field uses a dot-separated hierarchy matching the K/P/S convention from HTML IDs:

| HTML anchor        | Path encoding     | Meaning                    |
| ------------------ | ----------------- | -------------------------- |
| `name="K2P3"`      | `kap2.§3.st1`    | Chapter 2, Section 3, Stycke 1 |
| `name="K2P3S2"`    | `kap2.§3.st2`    | Chapter 2, Section 3, Stycke 2 |
| `name="P5"` (no K) | `kap0.§5.st1`    | No chapter, Section 5, Stycke 1 |
| (implicit chapter) | `kap0.§2a.st1`   | No chapter, Section 2a, Stycke 1 |

Convention: `kap0` means the document has no chapters (implicit single chapter).

### Contextual Header Examples

The `contextualHeader` is the breadcrumb used for contextual retrieval — it is prepended to the chunk text before embedding (Story 14.3).

```
// Law with chapters:
"Arbetsmiljolagen (SFS 1977:1160) > Kap 2: Arbetsmiljons beskaffenhet > 3 §"

// Law without chapters:
"Yrkestrafiklagen (SFS 2012:210) > 5 §"

// Amendment:
"SFS 2025:732 (andring av Arbetsmiljolagen) > Kap 6 > 17 §"

// Agency regulation:
"AFS 2023:1 Arbetsplatsens utformning > Kap 3: Ventilation > 15 §"
```

### ContentChunk Design Rationale

The `ContentChunk` model is designed to hold chunks from multiple source types:

| sourceType        | sourceId points to     | workspaceId | When chunked                   |
| ----------------- | ---------------------- | ----------- | ------------------------------ |
| `LEGAL_DOCUMENT`  | `LegalDocument.id`     | NULL        | This story (14.2)              |
| `USER_FILE`       | Future UserFile.id     | Set         | Future story (14.5)            |
| `CONVERSATION`    | Future Conversation.id | Set         | Future story (14.6)            |
| `ASSESSMENT`      | Future Assessment.id   | Set         | Future story                   |

This story only implements `LEGAL_DOCUMENT` chunking. The other source types are defined in the enum for forward compatibility.

### Prisma Transaction Pattern

The chunk sync uses a transaction to ensure atomicity (no partial state where old chunks are deleted but new ones fail to create):

```typescript
await prisma.$transaction([
  prisma.contentChunk.deleteMany({
    where: { source_type: 'LEGAL_DOCUMENT', source_id: documentId }
  }),
  prisma.contentChunk.createMany({
    data: newChunks
  })
])
```

### LegalDocument Schema Reference

```prisma
model LegalDocument {
  id               String       @id @default(uuid())
  content_type     ContentType  // SFS_LAW, SFS_AMENDMENT, AGENCY_REGULATION
  document_number  String       @unique
  title            String
  json_content     Json?        // Canonical schema from Story 14.1
  // ...
  @@map("legal_documents")
}
```
[Source: prisma/schema.prisma:281-321]

### Estimated Scale

- ~10,803 SFS laws x ~15 avg paragraphs = ~162,000 chunks
- ~24,932 amendments x ~5 avg paragraphs = ~125,000 chunks
- ~288 agency regulations x ~30 avg paragraphs = ~8,600 chunks
- **Total: ~295,000 chunks** (well within HNSW index performance range for <1M vectors)

## Testing

**Test location:** `tests/unit/chunks/` [mirroring `lib/chunks/` source structure]

**Test framework:** Vitest. Mock Prisma client for sync tests; no mocking needed for pure chunking logic.

**Test files:**

- `tests/unit/chunks/chunk-document.test.ts` (at least 7 tests)
- `tests/unit/chunks/sync-document-chunks.test.ts` (at least 3 tests)

**Chunking function tests (`chunk-document.test.ts`):**
- Document with 2 chapters, 3 sections each, 2 paragraphs each → 12 chunks with correct paths
- Document with implicit chapter (number: null) → chunks use `kap0` in path, header omits chapter
- Contextual header format: verify breadcrumb includes title, chapter, section number
- Path encoding: verify `kap2.§3.st1` format for known input
- Content role propagation: paragraph with role `ALLMANT_RAD` → chunk has `contentRole: ALLMANT_RAD`
- Token count: verify `tokenCount` is set and approximately correct
- Empty json_content (no chapters) → returns empty array

**Sync function tests (`sync-document-chunks.test.ts`):**
- Happy path: document with json_content → deletes old chunks, creates new ones, returns stats
- Document without json_content → logs warning, returns zero counts
- Transaction atomicity: verify `$transaction` is called with delete + createMany (mock Prisma)

**Mocking strategy:** Mock `@/lib/prisma` using `vi.mock()` for sync tests. Use `vi.fn()` for `$transaction`, `contentChunk.deleteMany`, `contentChunk.createMany`, `legalDocument.findUnique`. [Source: established pattern from Story 8.15 tests]

## Change Log

| Date       | Version | Description            | Author     |
| ---------- | ------- | ---------------------- | ---------- |
| 2026-02-18 | 1.0     | Initial story creation | Sarah (PO) |

## Dev Agent Record

### Agent Model Used

### Debug Log References

### Completion Notes List

### File List

## QA Results
