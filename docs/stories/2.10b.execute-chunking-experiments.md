# Story 2.10b: Execute RAG Chunking Strategy Experiments

## Status

Draft

## Story

**As a** developer,
**I want** to run the chunking strategy experiments and analyze results,
**so that** I can determine the optimal chunking approach for each content type.

## Acceptance Criteria

1. All chunking strategies from Story 2.10a tested against test dataset
2. Experiments run for all combinations: strategy × content type × overlap
3. Embeddings generated for all chunk variants (using OpenAI text-embedding-3-small)
4. All 50 test queries executed against each strategy
5. Metrics collected for all experiments: Precision@K, Recall@K, F1, latency
6. Manual semantic coherence evaluation completed (sample 20 chunks per strategy)
7. RAG answer quality evaluated with GPT-4 (all 50 queries per strategy)
8. Results saved to structured format (JSON/CSV with all metrics)
9. Comparison visualizations generated (charts, tables, heat maps)
10. Analysis document created summarizing findings
11. Recommendation made for production chunking strategy per content type
12. Experiments complete in <48 hours runtime
13. Total experimental cost tracked (embeddings + GPT-4 evals < €200)

## Tasks / Subtasks

- [ ] Execute chunking experiments (AC: 1, 2, 3)
  - [ ] Run test harness from Story 2.10a
  - [ ] Generate embeddings for all chunk variants
  - [ ] Handle OpenAI rate limits (batch if needed)
  - [ ] Log all intermediate results

- [ ] Run test queries (AC: 4, 5)
  - [ ] Execute all 50 queries against each strategy
  - [ ] Measure retrieval metrics (P@K, R@K, F1)
  - [ ] Measure retrieval latency
  - [ ] Store results per query-strategy combination

- [ ] Manual semantic evaluation (AC: 6)
  - [ ] Sample 20 random chunks per strategy
  - [ ] Score 1-5 for semantic coherence
  - [ ] Document issues (cut-off sentences, missing context)
  - [ ] Calculate average coherence score

- [ ] RAG quality evaluation (AC: 7)
  - [ ] For each strategy, generate answers to all 50 queries
  - [ ] Use GPT-4 to evaluate answer quality (1-10 scale)
  - [ ] Measure: accuracy, completeness, citation quality
  - [ ] Average scores per strategy

- [ ] Generate visualizations (AC: 9)
  - [ ] Bar charts: F1 score by strategy
  - [ ] Line charts: Precision/Recall trade-offs
  - [ ] Heat map: Performance by content type × strategy
  - [ ] Scatter plot: Cost vs. quality

- [ ] Analyze results and make recommendation (AC: 10, 11)
  - [ ] Compare strategies across all metrics
  - [ ] Identify best strategy per content type
  - [ ] Document trade-offs and edge cases
  - [ ] Write recommendation with justification

- [ ] Track costs (AC: 13)
  - [ ] Embedding generation costs
  - [ ] GPT-4 evaluation costs
  - [ ] Total experimental budget

## Dev Notes

### Experimental Execution Plan

**Phase 1: Chunking & Embedding (24 hours)**

```typescript
const strategies = loadStrategies() // from 2.10a
const documents = loadTestDataset() // from 2.10a

for (const strategy of strategies) {
  console.log(`Chunking with strategy: ${strategy.name}`)

  const chunks = await chunkAllDocuments(documents, strategy)
  console.log(`Generated ${chunks.length} chunks`)

  // Generate embeddings in batches
  const embeddings = await generateEmbeddingsBatch(chunks, {
    model: 'text-embedding-3-small',
    batchSize: 100, // respect rate limits
    retries: 3,
  })

  await saveChunksAndEmbeddings(strategy.name, chunks, embeddings)
}
```

**Phase 2: Retrieval Testing (12 hours)**

```typescript
const queries = loadTestQueries() // from 2.10a

for (const strategy of strategies) {
  const embeddings = await loadEmbeddings(strategy.name)

  for (const query of queries) {
    const startTime = Date.now()

    // Generate query embedding
    const queryEmbedding = await embed(query.text)

    // Vector similarity search
    const retrieved = await searchSimilar(queryEmbedding, embeddings, (k = 10))

    const latency = Date.now() - startTime

    // Calculate metrics
    const metrics = {
      precision_at_5: calculatePrecision(
        retrieved.slice(0, 5),
        query.relevantChunks
      ),
      recall_at_5: calculateRecall(retrieved.slice(0, 5), query.relevantChunks),
      precision_at_10: calculatePrecision(retrieved, query.relevantChunks),
      recall_at_10: calculateRecall(retrieved, query.relevantChunks),
      latency_ms: latency,
    }

    await saveResult({
      strategy: strategy.name,
      query: query.id,
      ...metrics,
    })
  }
}
```

**Phase 3: RAG Quality Evaluation (8 hours)**

```typescript
for (const strategy of strategies) {
  const embeddings = await loadEmbeddings(strategy.name)

  for (const query of queries) {
    // Retrieve chunks
    const chunks = await retrieveTopK(query, embeddings, (k = 5))

    // Generate RAG answer
    const answer = await generateRAGAnswer(query.text, chunks)

    // Evaluate with GPT-4
    const evaluation = await evaluateWithGPT4({
      query: query.text,
      retrievedChunks: chunks,
      generatedAnswer: answer,
      expectedAnswer: query.expectedAnswer, // if available
    })

    await saveEvaluation({
      strategy: strategy.name,
      query: query.id,
      accuracy: evaluation.accuracy,
      completeness: evaluation.completeness,
      citations: evaluation.citations,
      overall: evaluation.overall,
    })
  }
}
```

**Phase 4: Manual Evaluation (4 hours)**

```typescript
// Human review of chunk quality
for (const strategy of strategies) {
  const chunks = await loadChunks(strategy.name)
  const sample = randomSample(chunks, 20)

  console.log(`\nManual Review: ${strategy.name}`)
  console.log('Rate each chunk 1-5 for semantic coherence:\n')

  const scores = []
  for (const chunk of sample) {
    console.log(chunk.text)
    const score = await promptUser('Score (1-5): ')
    scores.push(score)
  }

  const avgScore = average(scores)
  await saveManualEvaluation(strategy.name, avgScore, scores)
}
```

### GPT-4 Evaluation Prompt

```typescript
const evaluationPrompt = `You are evaluating a RAG (Retrieval-Augmented Generation) answer.

**User Query:** "${query}"

**Retrieved Chunks:**
${chunks.map((c, i) => `[${i + 1}] ${c.source}: ${c.text}`).join('\n\n')}

**Generated Answer:**
"${answer}"

${expectedAnswer ? `**Expected Answer (reference):**\n"${expectedAnswer}"` : ''}

**Evaluate on a scale of 1-10:**

1. **Accuracy:** Is the answer factually correct based on the retrieved chunks?
2. **Completeness:** Does it fully answer the question?
3. **Citations:** Are sources properly cited and relevant?

**Respond ONLY with valid JSON:**
{
  "accuracy": <1-10>,
  "completeness": <1-10>,
  "citations": <1-10>,
  "overall": <1-10>,
  "reasoning": "<brief explanation>"
}
`
```

### Expected Findings to Analyze

**What we're looking for:**

1. **Does structural chunking (by §) beat fixed token chunking for laws?**
   - Hypothesis: Yes, because legal structure is semantically meaningful
   - Metric to check: F1 score, semantic coherence

2. **What's the optimal chunk size?**
   - Compare 300, 500, 800, 1000 tokens
   - Trade-off: Precision (smaller) vs. Context (larger)
   - Metric to check: Precision@K vs. Recall@K curves

3. **Does overlap help?**
   - Compare 0, 25, 50, 100 token overlap
   - Expected: Some overlap improves recall without hurting precision
   - Metric to check: Recall improvement vs. storage cost

4. **Do different content types need different strategies?**
   - Laws vs. Court cases vs. EU documents
   - Metric to check: Performance heat map by type

5. **What's the cost-quality trade-off?**
   - More chunks = higher embedding cost
   - Metric to check: Scatter plot of cost vs. F1 score

### Visualization Examples

**1. Strategy Comparison (Bar Chart):**

```
F1 Score by Strategy
━━━━━━━━━━━━━━━━━━━━
Structural §   ████████████████ 0.82
Fixed 500      ███████████████  0.75
Fixed 800      ██████████████   0.71
Semantic Para  █████████████    0.68
Fixed 300      ████████████     0.63
```

**2. Content Type Performance (Heat Map):**

```
Strategy         | SFS Laws | Court Cases | EU Docs
─────────────────|──────────|─────────────|─────────
Structural §     |   0.85   |    0.72     |  0.79
Fixed 500        |   0.76   |    0.78     |  0.73
Semantic Para    |   0.69   |    0.81     |  0.65
```

**3. Cost vs. Quality (Scatter):**

```
RAG Quality (0-10)
10 │         ● Structural §
9  │       ●   Fixed 800
8  │     ●     Fixed 500
7  │   ●
6  │ ●         Fixed 300
   └─────────────────────
     €0.01  €0.02  €0.03
     Cost per document
```

### Analysis Document Structure

**chunking-strategy-analysis.md:**

```markdown
# RAG Chunking Strategy Experimental Results

## Executive Summary

- **Winner for SFS Laws:** [Strategy name] with F1=0.XX
- **Winner for Court Cases:** [Strategy name] with F1=0.XX
- **Winner for EU Docs:** [Strategy name] with F1=0.XX
- **Overall Cost:** €XXX for XX,XXX document embeddings

## Detailed Findings

### 1. Retrieval Accuracy

[Charts and analysis]

### 2. Semantic Coherence

[Manual evaluation results]

### 3. RAG Answer Quality

[GPT-4 evaluation results]

### 4. Performance & Cost

[Latency and cost analysis]

## Recommendation

Based on experimental data, we recommend:

- **SFS Laws:** [Strategy] because [reasoning]
- **Court Cases:** [Strategy] because [reasoning]
- **EU Documents:** [Strategy] because [reasoning]

## Implementation Plan

Story 2.10c will implement the chosen strategies.
```

### Important Architecture References

**From Architecture Section 3 (RAG Strategy):**

- OpenAI embedding model specs
- Vector similarity search algorithms

**From Story 2.10a:**

- Test dataset and queries
- Evaluation framework

### Testing

**Validation Checks:**

- All experiments complete successfully
- Results files generated and parseable
- Visualizations render correctly
- Analysis document has clear recommendations
- Costs within budget (< €200)

### User Responsibilities

- Review final recommendations
- Approve chosen strategy before Story 2.10c implementation

### Developer Responsibilities

- Execute all experiments systematically
- Ensure data quality and reproducibility
- Generate clear visualizations
- Write analysis with actionable recommendations
- Present findings to team for approval

## Change Log

| Date       | Version | Description                              | Author     |
| ---------- | ------- | ---------------------------------------- | ---------- |
| 2025-11-11 | 1.0     | Initial story creation (split from 2.10) | Sarah (PO) |

## Dev Agent Record

_To be populated by dev agent_

## QA Results

_To be populated by QA agent_
