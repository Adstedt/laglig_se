# Story 14.13: Retrieval Ground Truth Labeling & Accuracy Measurement

## Status

Approved

## Story

**As a** developer optimizing retrieval quality,
**I want** human-labeled ground truth judgments for the benchmark query set and automated accuracy metrics,
**so that** we can measure real retrieval accuracy (not just model scores), identify failure patterns, and make a data-driven decision on whether BM25 hybrid search is needed.

## Context & Dependencies

**Why this story exists:**

Story 14.3 built the embedding pipeline and Cohere Rerank v4 integration. The 55-query benchmark shows strong rerank scores (avg 0.87 relevance), but these are **model confidence scores, not accuracy measurements**. A 0.87 rerank score could still return the wrong document. Without human-labeled relevance judgments, we cannot compute real metrics (MRR, precision@k, nDCG) or identify what types of queries fail.

This story establishes the permanent retrieval evaluation harness: label once, re-run after every pipeline change. It also produces the data needed for a go/no-go decision on BM25 hybrid search.

**Builds on:**

- Story 14.3 (Embedding Generation Pipeline) — provides `scripts/benchmark-retrieval.ts` with 55 queries across 5 personas, `lib/search/rerank.ts` (Cohere Rerank v4), `lib/chunks/embed-chunks.ts` (OpenAI embeddings), pgvector HNSW index on `content_chunks`
- Existing benchmark infrastructure: `UserQuery` interface with `query` + `persona`, `searchSimilar()` function, `--compact` / `--rerank` / `--rerank-model` / `--rerank-top` CLI flags

**Depends on:** Story 14.3 must be complete (embeddings populated, rerank module working, benchmark script runnable).

**Depended on by:**

- Story 14.8 (RAG Retrieval Pipeline) — ground truth metrics inform retrieval confidence thresholds
- Potential Story 14.14 (BM25 Hybrid Search) — go/no-go decision produced by this story's analysis

**Key data points:**

| Metric | Vector Only | Vector + Rerank |
|--------|------------|-----------------|
| Avg top-1 score | 0.6338 (cosine sim) | 0.8735 (relevance) |
| Scenario queries (weakest) | 0.5579 | 0.8162 |
| Latency per rerank | — | 420ms avg |

These scores look good but we don't know if they correspond to correct answers.

**Query set size:** 55 queries is directionally sufficient for the go/no-go decision on hybrid search — failure patterns will be clearly visible. The evaluation harness supports expanding the query set post-MVP for tighter confidence intervals if needed.

## Acceptance Criteria

### Labeling Tool

1. Interactive CLI script `scripts/label-retrieval-results.ts` that presents each of the 55 benchmark queries with their top-5 results (from vector+rerank mode) to a human reviewer
2. For each result, reviewer rates: `relevant` (directly answers the query), `partial` (related but not a direct answer), or `not-relevant` (wrong document)
3. Reviewer can also flag a query as `no-answer-in-corpus` when the correct document doesn't exist in the database
4. Script supports resume — can stop and continue labeling from where you left off
5. Labels are persisted incrementally (saved after each query, not only at the end)

### Ground Truth Data

6. Labels stored in `data/retrieval-ground-truth.json` — this file becomes the permanent test harness
7. Schema includes: query text, persona, expected relevance per result (with document identifiers), reviewer notes, and `no-answer-in-corpus` flag
8. Ground truth file is committed to the repository (not gitignored) — it's a test artifact

### Evaluation Script

9. Script `scripts/evaluate-retrieval.ts` runs the full benchmark pipeline and scores against ground truth labels
10. Computes metrics: **MRR** (Mean Reciprocal Rank), **precision@1**, **precision@5**, **nDCG@5** — per persona and overall
11. Supports evaluation modes via CLI flags: `--mode vector` (vector only), `--mode rerank` (vector + rerank)
12. Displays a basic miss list: for each query where top-1 is `not-relevant`, show what was returned vs what the reviewer labeled as relevant

### Failure Analysis & Decision Gate

13. When run with `--analyze` flag, the evaluation output adds a failure categorization — each missed query classified as:
    - **keyword-gap**: Query uses exact identifiers or terms (SFS numbers, specific legal terms) that keyword search would catch
    - **semantic-gap**: Query meaning is clear but embeddings don't capture it (vocabulary mismatch that reranker also missed)
    - **corpus-gap**: The correct document is not in the database
    - **ranking-gap**: Correct document was in the over-fetched candidates but reranker didn't promote it
14. Summary table shows failure type distribution — this directly informs the hybrid search decision
15. A written recommendation section in the output: if >30% of failures are `keyword-gap`, recommend BM25 hybrid search; if failures are dominated by `corpus-gap` or `semantic-gap`, recommend alternative investments

### Testing

16. Unit tests for the evaluation metric functions (MRR, precision@k, nDCG computation) with known inputs/outputs
17. At least 6 unit tests

## Tasks / Subtasks

- [ ] **Task 1: Define ground truth schema** (AC: 6-8)
  - [ ] Design JSON schema for `data/retrieval-ground-truth.json`
  - [ ] Include: query, persona, results array (each with document identifier, contextual_header, relevance label), reviewer notes, no-answer-in-corpus flag, labeled timestamp
  - [ ] Create empty scaffold file with schema documentation comment

- [ ] **Task 2: Build labeling CLI tool** (AC: 1-5)
  - [ ] Create `scripts/label-retrieval-results.ts`
  - [ ] For each query: run vector+rerank search (reusing existing benchmark infrastructure), display query + persona + top-5 results with content snippets
  - [ ] Prompt reviewer for relevance rating per result: `[r]elevant / [p]artial / [n]ot-relevant`
  - [ ] Prompt for optional reviewer notes and no-answer-in-corpus flag
  - [ ] Save labels incrementally to `data/retrieval-ground-truth.json` after each query
  - [ ] Support `--resume` flag — skip already-labeled queries
  - [ ] Support `--query N` flag — jump to a specific query number for re-labeling

- [ ] **Task 3: Build evaluation metric functions** (AC: 9-11, 16-17)
  - [ ] Create `lib/search/eval-metrics.ts` with pure functions:
    - `computeMRR(rankings: RelevanceLabel[][]): number`
    - `computePrecisionAtK(rankings: RelevanceLabel[][], k: number): number`
    - `computeNDCG(rankings: RelevanceLabel[][], k: number): number`
  - [ ] Relevance scores for nDCG: `relevant` = 2, `partial` = 1, `not-relevant` = 0
  - [ ] Unit tests in `tests/unit/search/eval-metrics.test.ts` — at least 6 tests covering edge cases (perfect ranking, worst ranking, single result, mixed relevance)

- [ ] **Task 4: Build evaluation script** (AC: 9-12)
  - [ ] Create `scripts/evaluate-retrieval.ts`
  - [ ] Load ground truth from `data/retrieval-ground-truth.json`
  - [ ] Run benchmark in specified mode (`--mode vector` or `--mode rerank`)
  - [ ] Match results to ground truth labels by stable identifier — store the first 100 chars of `contextual_header` as the match key in ground truth (full headers may shift if content is re-chunked). Alternatively, store chunk `id` for exact matching if available
  - [ ] Compute and display: MRR, precision@1, precision@5, nDCG@5 — overall and per persona
  - [ ] Display comparison table when run in both modes

- [ ] **Task 5: Build failure analysis** (AC: 13-15)
  - [ ] Extend evaluation script with `--analyze` flag
  - [ ] For each query where top-1 is not `relevant`: display the miss with returned vs expected
  - [ ] Categorize each failure: `keyword-gap`, `semantic-gap`, `corpus-gap`, `ranking-gap`
  - [ ] Print summary distribution table of failure types
  - [ ] Print recommendation: if keyword-gap > 30% of failures → recommend BM25, otherwise → recommend alternative

- [ ] **Task 6: Run labeling session** (AC: 1-8)
  - [ ] Requires live database connection, `OPENAI_API_KEY`, and `COHERE_API_KEY` in `.env.local`
  - [ ] Human reviewer labels all 55 queries (~1-2 hours)
  - [ ] Commit `data/retrieval-ground-truth.json` to repository
  - [ ] Document any queries where correct answer is ambiguous

- [ ] **Task 7: Run evaluation & produce decision** (AC: 9-15)
  - [ ] Run `scripts/evaluate-retrieval.ts --mode vector`
  - [ ] Run `scripts/evaluate-retrieval.ts --mode rerank`
  - [ ] Run `scripts/evaluate-retrieval.ts --mode rerank --analyze`
  - [ ] Document results in Dev Notes section of this story
  - [ ] Write go/no-go recommendation for BM25 hybrid search

## Dev Notes

### Source Tree

```
lib/search/
  ├── rerank.ts                    — FROM Story 14.3 — Cohere Rerank v4
  ├── eval-metrics.ts              — NEW — MRR, precision@k, nDCG pure functions
  └── index.ts                     — MODIFIED — add eval-metrics exports

scripts/
  ├── benchmark-retrieval.ts       — FROM Story 14.3 — 55-query benchmark (READ ONLY)
  ├── label-retrieval-results.ts   — NEW — interactive labeling CLI
  └── evaluate-retrieval.ts        — NEW — automated evaluation with metrics

data/
  └── retrieval-ground-truth.json  — NEW — human-labeled relevance judgments (committed)

tests/unit/search/
  ├── rerank.test.ts               — FROM Story 14.3 (14 tests)
  └── eval-metrics.test.ts         — NEW — metric computation tests
```

### Existing Benchmark Infrastructure (from 14.3)

The benchmark script at `scripts/benchmark-retrieval.ts` provides:

- 55 queries across 5 personas: HR (14), Arbetsmiljö (13), Compliance (14), IT (3), Scenario (11)
- `searchSimilar(embedding, topK, fullContent)` — pgvector cosine search with HNSW index
- `rerank()` from `lib/search/rerank.ts` — Cohere v4 cross-encoder, 4x over-fetch pattern
- `buildRerankText()` — combines contextual_header + context_prefix + content for reranker input
- CLI flags: `--top N`, `--compact`, `--rerank`, `--rerank-model`, `--rerank-top`

The labeling and evaluation scripts should **reuse** the existing search functions rather than duplicating them. Consider extracting shared search logic into `lib/search/` if needed.

### Metric Definitions

**MRR (Mean Reciprocal Rank):** Average of 1/rank of the first relevant result. MRR=1.0 means the correct answer is always #1. A query with no relevant results in top-K contributes 0.

**Precision@k:** Fraction of top-k results that are relevant (counting `relevant` and `partial`).

**nDCG@5 (Normalized Discounted Cumulative Gain):** Measures ranking quality with position-aware weighting. Uses graded relevance: `relevant`=2, `partial`=1, `not-relevant`=0. A perfect nDCG@5 of 1.0 means all relevant documents are at the top in ideal order.

### Ground Truth JSON Schema

```json
{
  "version": 1,
  "labeledAt": "2026-02-27T...",
  "reviewer": "audri",
  "queries": [
    {
      "queryIndex": 0,
      "query": "Vi ska anställa en person på deltid...",
      "persona": "HR",
      "noAnswerInCorpus": false,
      "reviewerNotes": "",
      "results": [
        {
          "contextualHeader": "Lag (1982:80) om anställningsskydd > ...",
          "relevance": "relevant"
        },
        {
          "contextualHeader": "...",
          "relevance": "partial"
        }
      ]
    }
  ]
}
```

### Failure Type Classification Guide

For Task 5, use these heuristics to classify failures:

- **keyword-gap:** The query contains specific identifiers (SFS numbers, AFS codes, "6 kap. 3 §") or exact legal terms that a keyword/BM25 search would match but vector search misses
- **semantic-gap:** The query is natural language, the vocabulary gap is too large for both embeddings and the reranker (e.g., very colloquial Swedish vs formal legal language)
- **corpus-gap:** The correct law or regulation is simply not in the database (like the visselblåsar/whistleblower example scoring 0.329)
- **ranking-gap:** The correct document appears in the over-fetched candidates (top 20) but the reranker fails to promote it to top 5

### Decision Gate Thresholds

These thresholds are team-defined pragmatic heuristics for this project, not industry-standard benchmarks. They can be adjusted based on the actual distribution of failures observed. After evaluation, apply these decision criteria:

| Failure distribution | Recommendation |
|---------------------|---------------|
| >30% keyword-gap | Build BM25 hybrid search (Story 14.14) |
| >30% corpus-gap | Prioritize ingesting missing documents |
| >30% semantic-gap | Investigate better embedding model or fine-tuning |
| >30% ranking-gap | Tune reranker (different model, adjust top_n) |
| Mixed / low failure rate | Current pipeline is sufficient for MVP |

### CLI Usage

```bash
# Step 1: Label (interactive, ~1-2 hours)
npx tsx scripts/label-retrieval-results.ts
npx tsx scripts/label-retrieval-results.ts --resume          # continue from where you stopped

# Step 2: Evaluate
npx tsx scripts/evaluate-retrieval.ts --mode vector          # vector-only baseline
npx tsx scripts/evaluate-retrieval.ts --mode rerank          # vector + rerank
npx tsx scripts/evaluate-retrieval.ts --mode rerank --analyze # with failure analysis

# Re-label a specific query
npx tsx scripts/label-retrieval-results.ts --query 7
```

### Testing

**Test location:** `tests/unit/search/` [mirroring `lib/search/` source structure]

**Test framework:** Vitest with `vi.fn()` mocks.

**Test file:** `tests/unit/search/eval-metrics.test.ts`

**Minimum 6 unit tests for metric functions:**
1. MRR with perfect ranking (first result always relevant) → 1.0
2. MRR with mixed results (relevant at position 2, 3, etc.)
3. MRR with no relevant results → 0.0
4. Precision@1 and Precision@5 with known inputs
5. nDCG@5 with perfect ranking → 1.0
6. nDCG@5 with graded relevance (mix of relevant, partial, not-relevant)

**No mocking of external APIs needed** — metric functions are pure math. The labeling and evaluation scripts are CLI tools verified by running them, not unit tests.

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2026-02-26 | 1.0 | Initial story creation — ground truth labeling and retrieval accuracy measurement with BM25 decision gate | Bob (SM) |
| 2026-02-26 | 1.1 | PO validation fixes: clarified AC 12 vs AC 13 scope (miss list vs categorization), added runtime deps to Task 6, stabilized ground truth matching strategy, documented threshold heuristics as team-defined, added query set expandability note | Sarah (PO) |

## Dev Agent Record

### Agent Model Used

### Debug Log References

### Completion Notes List

### File List

## QA Results
