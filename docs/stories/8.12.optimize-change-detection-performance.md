# Story 8.12: Optimize Change Detection Performance

## Status
Draft

## Story

**As a** product owner,
**I want** to ensure change detection cron job completes within 2 hours,
**so that** notifications are timely.

## Acceptance Criteria

1. **Daily cron job optimized:**
   - Parallel processing: 10 laws concurrently
   - Incremental hashing: Compare checksums before full diff
   - Rate limiting: Respect Riksdagen API limits (10 req/sec)
   - Batch processing: Process laws in batches of 100

2. **Job completion time <2 hours (target, NFR10)**
   - For 10,000 laws in database
   - On production infrastructure

3. **Error handling:**
   - Retry failed fetches up to 3 times
   - Log errors to Sentry
   - Continue processing other laws if one fails

4. **Progress tracking:**
   - Log progress every 1,000 laws: "Processed 5,000/10,000 laws (50%)"
   - Store last successful run timestamp
   - Dashboard shows job status and duration

5. **Alerting:**
   - Email founder if job fails
   - Email founder if job exceeds 3 hours
   - Slack notification for job failures (optional)

6. **Performance dashboard:**
   - Job runtime trend chart (last 30 days)
   - Laws processed per minute
   - Error rate percentage
   - Average diff generation time

7. **Optimization techniques:**
   - Cache frequently accessed laws
   - Skip laws with no recent Riksdagen activity
   - Prioritize high-traffic laws first
   - Database indexes on key fields

8. **Monitoring:**
   - Datadog APM for performance metrics
   - Vercel Cron logs for execution history
   - OpenAI token usage tracking

## Tasks / Subtasks

- [ ] Implement parallel processing (10 concurrent)
- [ ] Add incremental hashing with checksums
- [ ] Implement rate limiting (10 req/sec)
- [ ] Add batch processing (100 laws per batch)
- [ ] Implement retry logic with exponential backoff
- [ ] Add error logging to Sentry
- [ ] Implement progress logging (every 1,000 laws)
- [ ] Add alerting for failures and timeouts
- [ ] Create performance dashboard
- [ ] Add database indexes
- [ ] Implement caching for frequently accessed laws
- [ ] Test with 10,000+ laws
- [ ] Optimize diff generation algorithm
- [ ] Monitor OpenAI token usage

## Dev Notes

**Optimized Change Detection Cron Job:**
```typescript
// app/api/cron/detect-law-changes/route.ts (optimized version)
import { NextResponse } from 'next/server'
import { prisma } from '@/lib/prisma'
import { generateAISummaryWithRetry } from '@/lib/law-changes/generate-ai-summary'
import { detectEffectiveDate } from '@/lib/law-changes/detect-effective-date'
import * as Sentry from '@sentry/nextjs'
import pLimit from 'p-limit'
import crypto from 'crypto'

const CONCURRENCY_LIMIT = 10 // Process 10 laws in parallel
const RATE_LIMIT_MS = 100 // 100ms between batches = 10 req/sec
const BATCH_SIZE = 100 // Process 100 laws per batch
const MAX_RETRIES = 3

interface JobStats {
  totalLaws: number
  processed: number
  changesDetected: number
  errors: number
  skippedNoChange: number
  startTime: number
}

export async function GET(request: Request) {
  // Verify Vercel Cron secret
  const authHeader = request.headers.get('authorization')
  if (authHeader !== `Bearer ${process.env.CRON_SECRET}`) {
    return NextResponse.json({ error: 'Unauthorized' }, { status: 401 })
  }

  const stats: JobStats = {
    totalLaws: 0,
    processed: 0,
    changesDetected: 0,
    errors: 0,
    skippedNoChange: 0,
    startTime: Date.now()
  }

  try {
    // Fetch all laws with last_checked_hash
    const laws = await prisma.law.findMany({
      select: {
        id: true,
        sfsNummer: true,
        rubrik: true,
        lastCheckedHash: true,
        lastCheckedAt: true
      },
      orderBy: { lastCheckedAt: 'asc' } // Prioritize least recently checked
    })

    stats.totalLaws = laws.length
    console.log(`Starting change detection for ${stats.totalLaws} laws`)

    // Process in batches
    for (let i = 0; i < laws.length; i += BATCH_SIZE) {
      const batch = laws.slice(i, i + BATCH_SIZE)
      console.log(`Processing batch ${Math.floor(i / BATCH_SIZE) + 1}/${Math.ceil(laws.length / BATCH_SIZE)}`)

      // Process batch in parallel with concurrency limit
      const limit = pLimit(CONCURRENCY_LIMIT)
      const promises = batch.map((law) =>
        limit(() => processLaw(law, stats))
      )

      await Promise.allSettled(promises)

      // Progress logging every 1,000 laws
      if (stats.processed % 1000 === 0 || stats.processed === stats.totalLaws) {
        const percentage = Math.round((stats.processed / stats.totalLaws) * 100)
        const elapsed = (Date.now() - stats.startTime) / 1000 / 60 // minutes
        const lawsPerMinute = stats.processed / elapsed
        console.log(
          `Progress: ${stats.processed}/${stats.totalLaws} (${percentage}%) | ` +
          `Elapsed: ${elapsed.toFixed(1)}m | ` +
          `Speed: ${lawsPerMinute.toFixed(0)} laws/min | ` +
          `Changes: ${stats.changesDetected} | ` +
          `Errors: ${stats.errors}`
        )
      }

      // Rate limiting: Wait between batches
      if (i + BATCH_SIZE < laws.length) {
        await new Promise((resolve) => setTimeout(resolve, RATE_LIMIT_MS))
      }
    }

    const durationMinutes = (Date.now() - stats.startTime) / 1000 / 60

    // Alert if exceeds 3 hours
    if (durationMinutes > 180) {
      await sendAlertEmail('Change detection job exceeded 3 hours', stats)
      Sentry.captureMessage('Change detection job exceeded 3 hours', {
        level: 'warning',
        extra: stats
      })
    }

    // Store job run record
    await prisma.cronJobRun.create({
      data: {
        jobName: 'detect-law-changes',
        startedAt: new Date(stats.startTime),
        completedAt: new Date(),
        durationMs: Date.now() - stats.startTime,
        status: durationMinutes < 120 ? 'success' : 'warning',
        metadata: stats
      }
    })

    console.log(`Change detection completed in ${durationMinutes.toFixed(1)} minutes`)

    return NextResponse.json({
      success: true,
      ...stats,
      durationMinutes,
      status: durationMinutes < 120 ? 'success' : 'warning'
    })
  } catch (error) {
    console.error('Change detection cron job failed:', error)
    Sentry.captureException(error, { extra: stats })
    await sendAlertEmail('Change detection job failed', stats)

    return NextResponse.json(
      {
        error: 'Cron job failed',
        message: error instanceof Error ? error.message : 'Unknown error',
        stats
      },
      { status: 500 }
    )
  }
}

async function processLaw(law: any, stats: JobStats): Promise<void> {
  let retries = 0

  while (retries < MAX_RETRIES) {
    try {
      // Fetch current version from Riksdagen
      const currentContent = await fetchLawContent(law.sfsNummer)

      // Generate hash
      const currentHash = crypto
        .createHash('sha256')
        .update(currentContent)
        .digest('hex')

      // Incremental check: Compare hashes
      if (currentHash === law.lastCheckedHash) {
        // No change, skip
        stats.skippedNoChange++
        stats.processed++

        // Update lastCheckedAt
        await prisma.law.update({
          where: { id: law.id },
          data: { lastCheckedAt: new Date() }
        })

        return
      }

      // Hash differs, generate full diff
      const oldContent = law.lastCheckedHash
        ? await fetchLawContentFromCache(law.id)
        : ''

      if (oldContent) {
        // Detect change
        await detectAndStoreChange(law, oldContent, currentContent)
        stats.changesDetected++
      }

      // Update law record
      await prisma.law.update({
        where: { id: law.id },
        data: {
          lastCheckedHash: currentHash,
          lastCheckedAt: new Date()
        }
      })

      // Cache current content for future comparisons
      await cacheLawContent(law.id, currentContent)

      stats.processed++
      return
    } catch (error) {
      retries++
      console.error(`Failed to process law ${law.sfsNummer} (attempt ${retries}):`, error)

      if (retries < MAX_RETRIES) {
        // Exponential backoff: 2^retries seconds
        await new Promise((resolve) => setTimeout(resolve, Math.pow(2, retries) * 1000))
      } else {
        // Max retries exceeded, log error and continue
        stats.errors++
        stats.processed++
        Sentry.captureException(error, {
          tags: { law: law.sfsNummer },
          extra: { lawId: law.id }
        })
        return
      }
    }
  }
}

async function fetchLawContent(sfsNummer: string): Promise<string> {
  // Fetch from Riksdagen API
  const response = await fetch(
    `https://data.riksdagen.se/dokument/${sfsNummer}.html`,
    { signal: AbortSignal.timeout(10000) } // 10s timeout
  )

  if (!response.ok) {
    throw new Error(`Failed to fetch law ${sfsNummer}: ${response.status}`)
  }

  return await response.text()
}

async function fetchLawContentFromCache(lawId: string): Promise<string> {
  // Fetch from cache (Redis or database)
  const cached = await prisma.lawContentCache.findUnique({
    where: { lawId }
  })

  return cached?.content || ''
}

async function cacheLawContent(lawId: string, content: string): Promise<void> {
  // Cache content for future comparisons
  await prisma.lawContentCache.upsert({
    where: { lawId },
    update: { content, updatedAt: new Date() },
    create: { lawId, content }
  })
}

async function detectAndStoreChange(
  law: any,
  oldContent: string,
  newContent: string
): Promise<void> {
  // Determine priority
  const priority = determinePriority(oldContent, newContent)

  // Extract affected sections
  const affectedSections = extractAffectedSections(oldContent, newContent)

  // Generate AI summary (with retry)
  const { aiSummary, businessImpact, contextualExplanation } =
    await generateAISummaryWithRetry(
      oldContent,
      newContent,
      law.rubrik,
      law.sfsNummer,
      priority,
      affectedSections
    )

  // Detect effective date
  const effectiveDate = await detectEffectiveDate(newContent, law.rubrik)

  // Store change
  await prisma.lawChange.create({
    data: {
      lawId: law.id,
      changeType: 'amendment',
      priority,
      detectedAt: new Date(),
      oldContent,
      newContent,
      diffContent: generateDiff(oldContent, newContent),
      aiSummary,
      businessImpact,
      contextualExplanation,
      affectedSections,
      effectiveDate,
      officialSourceUrl: `https://www.riksdagen.se/sv/dokument-och-lagar/dokument/svensk-forfattningssamling/${law.sfsNummer}`
    }
  })

  console.log(`Change detected for ${law.sfsNummer}`)
}

function determinePriority(oldContent: string, newContent: string): 'high' | 'medium' | 'low' {
  // Logic to determine priority based on change magnitude
  const changePercentage = calculateChangePercentage(oldContent, newContent)

  if (changePercentage > 20) return 'high'
  if (changePercentage > 5) return 'medium'
  return 'low'
}

function calculateChangePercentage(oldContent: string, newContent: string): number {
  const oldLines = oldContent.split('\n').length
  const newLines = newContent.split('\n').length
  const linesDiff = Math.abs(newLines - oldLines)
  return (linesDiff / oldLines) * 100
}

function extractAffectedSections(oldContent: string, newContent: string): string[] {
  // Extract section numbers mentioned in diff
  const sectionPattern = /ยง\s*\d+|(\d+\s*kap\.\s*\d+\s*ยง)/g
  const mentions = newContent.match(sectionPattern) || []
  return [...new Set(mentions)]
}

function generateDiff(oldContent: string, newContent: string): string {
  // Use diff library to generate diff
  const diff = require('diff')
  const changes = diff.diffLines(oldContent, newContent)
  return JSON.stringify(changes)
}

async function sendAlertEmail(subject: string, stats: JobStats): Promise<void> {
  // Send alert email to founder
  const founder = await prisma.user.findFirst({
    where: { role: 'founder' }
  })

  if (founder) {
    await fetch('/api/email/send-alert', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        to: founder.email,
        subject,
        stats
      })
    })
  }
}
```

**Database Schema for Job Tracking:**
```prisma
model CronJobRun {
  id          String   @id @default(uuid())
  jobName     String
  startedAt   DateTime
  completedAt DateTime?
  durationMs  Int?
  status      String   // success, warning, error
  metadata    Json?

  @@index([jobName, startedAt])
  @@map("cron_job_runs")
}

model LawContentCache {
  id        String   @id @default(uuid())
  lawId     String   @unique
  content   String   @db.Text
  updatedAt DateTime @updatedAt

  law Law @relation(fields: [lawId], references: [id])

  @@map("law_content_cache")
}
```

**Database Indexes:**
```prisma
model Law {
  // ... existing fields

  @@index([lastCheckedAt])
  @@index([lastCheckedHash])
  @@index([sfsNummer])
}

model LawChange {
  // ... existing fields

  @@index([lawId, acknowledgedAt])
  @@index([detectedAt])
  @@index([effectiveDate])
  @@index([priority])
}
```

**Performance Dashboard:**
```typescript
// app/admin/performance/page.tsx
import { prisma } from '@/lib/prisma'
import { PerformanceChart } from '@/components/admin/performance-chart'

export default async function PerformanceDashboard() {
  // Fetch last 30 days of job runs
  const jobRuns = await prisma.cronJobRun.findMany({
    where: {
      jobName: 'detect-law-changes',
      startedAt: {
        gte: new Date(Date.now() - 30 * 24 * 60 * 60 * 1000)
      }
    },
    orderBy: { startedAt: 'desc' }
  })

  const avgDuration = jobRuns.reduce((sum, run) => sum + (run.durationMs || 0), 0) / jobRuns.length
  const errorRate = (jobRuns.filter((run) => run.status === 'error').length / jobRuns.length) * 100

  return (
    <div className="max-w-7xl mx-auto p-6">
      <h1 className="text-2xl font-bold mb-6">Change Detection Performance</h1>

      {/* Stats */}
      <div className="grid grid-cols-4 gap-4 mb-8">
        <div className="bg-white rounded-lg shadow-sm border border-gray-200 p-4">
          <h3 className="text-sm font-medium text-gray-600">Average Duration</h3>
          <p className="text-2xl font-bold mt-1">{(avgDuration / 1000 / 60).toFixed(1)} min</p>
        </div>
        <div className="bg-white rounded-lg shadow-sm border border-gray-200 p-4">
          <h3 className="text-sm font-medium text-gray-600">Error Rate</h3>
          <p className="text-2xl font-bold mt-1">{errorRate.toFixed(1)}%</p>
        </div>
        <div className="bg-white rounded-lg shadow-sm border border-gray-200 p-4">
          <h3 className="text-sm font-medium text-gray-600">Last Run</h3>
          <p className="text-2xl font-bold mt-1">
            {jobRuns[0]?.startedAt ? new Date(jobRuns[0].startedAt).toLocaleString('sv-SE') : 'N/A'}
          </p>
        </div>
        <div className="bg-white rounded-lg shadow-sm border border-gray-200 p-4">
          <h3 className="text-sm font-medium text-gray-600">Total Runs</h3>
          <p className="text-2xl font-bold mt-1">{jobRuns.length}</p>
        </div>
      </div>

      {/* Chart */}
      <div className="bg-white rounded-lg shadow-sm border border-gray-200 p-6">
        <h3 className="text-lg font-semibold mb-4">Runtime Trend (Last 30 Days)</h3>
        <PerformanceChart jobRuns={jobRuns} />
      </div>
    </div>
  )
}
```

**Install p-limit for Concurrency Control:**
```bash
npm install p-limit
```

**Reference:** PRD Epic 8 Story 8.12, NFR10 (change detection <2 hours)

## Testing

**Unit Tests:**
- Parallel processing limits concurrency to 10
- Incremental hashing detects changes correctly
- Rate limiting enforces 10 req/sec
- Retry logic works with exponential backoff
- Progress logging occurs every 1,000 laws

**Integration Tests:**
- Full cron job completes successfully
- Changes stored in database correctly
- AI summaries generated for detected changes
- Job run record created in database
- Alert email sent if job exceeds 3 hours

**Load Testing:**
- Test with 1,000 laws: completes <10 minutes
- Test with 5,000 laws: completes <1 hour
- Test with 10,000 laws: completes <2 hours (target)
- Test with 20,000 laws: completes <4 hours

**Performance Testing:**
- Parallel processing achieves 10 req/sec throughput
- Incremental hashing skips 95%+ unchanged laws
- Diff generation <1 second per law
- AI summary generation <5 seconds per change
- Database queries optimized with indexes

**Error Handling Testing:**
- Simulate Riksdagen API failure, verify retry works
- Simulate OpenAI API failure, verify fallback works
- Simulate database timeout, verify error logged to Sentry
- Verify job continues processing after error

**Monitoring Testing:**
- Verify progress logs appear every 1,000 laws
- Verify job run record created in database
- Verify Sentry receives error logs
- Verify alert email sent if job exceeds 3 hours

**Edge Cases:**
- Law with no hash (first run, generate hash)
- Law not found on Riksdagen (log error, continue)
- Very large law (10MB+) (timeout, retry)
- OpenAI rate limit exceeded (exponential backoff)

**Test File:** `__tests__/features/cron/change-detection-performance.test.tsx`

## User vs Developer Responsibilities

**User Responsibility:**
- None (this is an internal performance optimization)

**Developer Responsibility:**
- Optimize cron job to complete <2 hours (NFR10)
- Implement parallel processing for throughput
- Use incremental hashing to skip unchanged laws
- Respect Riksdagen API rate limits (10 req/sec)
- Implement retry logic with exponential backoff
- Log errors to Sentry for debugging
- Track progress and log every 1,000 laws
- Alert founder if job fails or exceeds 3 hours
- Create performance dashboard for monitoring
- Add database indexes for query optimization
- Cache law content for future comparisons
- Monitor OpenAI token usage and costs
- Continuously optimize based on metrics
- Test with production-scale data (10,000+ laws)
- Ensure job completes within Vercel Cron timeout (10 minutes for Pro, 5 minutes for Hobby)

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|---------|
| 2025-11-12 | 1.0 | Initial story creation | Sarah (PO) |

## Dev Agent Record
_To be populated by dev agent_

## QA Results
_To be populated by QA agent_
